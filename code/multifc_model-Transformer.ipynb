{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fd615a-450f-46f1-ab39-b6fcef802df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url, Data, Batch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mendeleev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e36dfec-0763-4e55-be3a-e90b5ecd89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mendeleev import element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7e0a8f-6d98-4bf3-9295-b423e279dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, H, N, O, F = element([\"C\", \"H\", \"N\", \"O\", \"F\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b06396-df25-4d9b-bd0b-476c3107033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for e in [C, H, N, O, F]:\n",
    "    row = [e.atomic_radius,\n",
    "           e.atomic_volume,\n",
    "           e.atomic_weight,\n",
    "           e.boiling_point,\n",
    "           e.covalent_radius_bragg,\n",
    "           e.dipole_polarizability,\n",
    "           e.electron_affinity,\n",
    "           e.en_ghosh,\n",
    "           e.vdw_radius]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893b4d2d-655b-435e-b955-c06190188fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties=np.array(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d88aec5-a3e5-40f4-85f9-e4246345bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties[properties == None] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9022d475-1227-4fcf-9f82-f49b6174f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "emms = MinMaxScaler([-5, 5])\n",
    "props = properties.astype(np.float32)\n",
    "props = emms.fit_transform(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10764aa2-7d94-479d-b435-2b2625c90271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metaedge_features(incident_edges, coords):\n",
    "    head2tail = []\n",
    "    triads_ht = []\n",
    "    head2head = []\n",
    "    triads_hh = []\n",
    "    tail2tail = []\n",
    "    triads_tt = []\n",
    "    for x, i in enumerate(incident_edges):\n",
    "        for y, j in enumerate(incident_edges):\n",
    "            if i[1] == j[0]:\n",
    "                head2tail.append((x, y))\n",
    "                triads_ht.append((i[0], i[1], j[1]))\n",
    "            if i[0] == j[0] and x != y:\n",
    "                tail2tail.append((x, y))\n",
    "                triads_hh.append((i[1], i[0], j[1]))\n",
    "            if i[1] == j[1] and x != y:\n",
    "                head2head.append((x, y))\n",
    "                triads_tt.append((i[0], i[1], j[0]))\n",
    "\n",
    "    head2tail = np.array(head2tail)\n",
    "    head2head = np.array(head2head)\n",
    "    tail2tail = np.array(tail2tail)\n",
    "    all_triads = triads_ht + triads_hh + triads_tt\n",
    "    non_empty = [array for array in [head2tail, head2head, tail2tail] if len(array) > 0]\n",
    "    metaedges = np.concatenate(non_empty)\n",
    "    angles = []\n",
    "    for triad in all_triads:\n",
    "        angles.append(get_angle(coords, triad))\n",
    "    return metaedges, np.array(angles)\n",
    "\n",
    "def get_angle(array, triad):\n",
    "    \"\"\"\n",
    "    Array of distances to angles between edges\n",
    "    \"\"\"\n",
    "    i, j, k = triad\n",
    "    a =  array[i] - array[j]\n",
    "    b =  array[k] - array[j]\n",
    "    a = a/np.linalg.norm(a)\n",
    "    b = b/np.linalg.norm(b)\n",
    "    return a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11280e70-1350-4fdc-b5ea-594205942b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_features(coords, dipole_moment, eps = 0.000001):\n",
    "    norm_dipole = np.linalg.norm(dipole_moment) # get the norm to normalize the vector and find angles\n",
    "    distmat = squareform(pdist(coords)) # get dist_mat to find edges\n",
    "    np.fill_diagonal(distmat, np.nan) # fill to avoid ranking problem\n",
    "    rankings = distmat.argsort(axis=0) # order distance matrix to get n-neighborhood of each edge\n",
    "    G = nx.from_numpy_matrix(distmat, create_using=nx.DiGraph())\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) # remove to avoid self edges\n",
    "    edgelist = nx.to_edgelist(G)\n",
    "    edgelist, edge_features = edge_list_to_numpy(edgelist) #get edges and distances\n",
    "    G = nx.from_numpy_matrix(rankings+1, create_using=nx.DiGraph())\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    edgelist2 = nx.to_edgelist(G)\n",
    "    edgelist2, edge_rankings = edge_list_to_numpy(edgelist2) # get rankings\n",
    "    edge_rankings = edge_rankings - 1\n",
    "    n_edges = edgelist.shape[0]\n",
    "    coords_nodes = coords[edgelist] #select nodes in edges\n",
    "    vectors_edges = coords_nodes.transpose(0, 2, 1)[:, :, 1] - coords_nodes.transpose(0, 2, 1)[:, :, 0] # get vector of each edge\n",
    "    vectors_edges_normalized = vectors_edges/(np.linalg.norm(vectors_edges, axis=1) + eps)[:,None] \n",
    "    dipole_moment_normalized = dipole_moment/(norm_dipole + eps)\n",
    "    angles_dipole_moment = np.dot(vectors_edges_normalized, dipole_moment) # do dot product to find angles\n",
    "    ranks = np.zeros([n_edges, 6])\n",
    "    edge_rankings[edge_rankings > 4] = 5 # replace to impose same dimensionality in all molecules\n",
    "    ranks[np.arange(n_edges), edge_rankings] = 1\n",
    "    edge_features = np.concatenate([edge_features[:,None], ranks, angles_dipole_moment[:,None]], axis=1) # concatenate all features\n",
    "    return edgelist, edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10999a66-5f37-4f67-bf94-4b9b1ea21329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_list_to_numpy(edgelist):\n",
    "    tail = []\n",
    "    head = []\n",
    "    weight = []\n",
    "    for edge in list(edgelist):\n",
    "        tail.append(edge[0])\n",
    "        head.append(edge[1])\n",
    "        weight.append(edge[2][\"weight\"])\n",
    "    tail = np.array(tail)[:,None]\n",
    "    head = np.array(head)[:,None]\n",
    "    weight = np.array(weight)\n",
    "    return np.concatenate([tail, head], axis=1), weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce54f832-c04b-46b1-b7de-e8e123d761c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fefd4fb-b9cc-43f7-b411-2ad0017f6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = target[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7f5252-fc4d-47fb-92c0-7104debc580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_to_int = {edges[i]:i for i in range(len(edges))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1629b297-4240-4bf6-9d19-c6e7c60a01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, training= True):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.filenames = pd.read_csv(\"raw/processed_names.csv\")\n",
    "        self.charges = pd.read_csv(\"raw/mulliken_charges.csv\")\n",
    "        self.magnetic_shieldings = pd.read_csv(\"raw/magnetic_shielding_tensors.csv\")\n",
    "        self.dipole_moments = pd.read_csv(\"raw/dipole_moments.csv\")\n",
    "        self.potential_energy = pd.read_csv(\"raw/potential_energy.csv\")\n",
    "        self.target = pd.read_csv(\"raw/train.csv\")\n",
    "        self.structures = pd.read_csv(\"raw/structures.csv\")\n",
    "        self.molecule_names = molecule_names = np.unique(self.potential_energy[\"molecule_name\"])\n",
    "        if training:\n",
    "            self.training_mask = np.loadtxt(\"./raw/training_mask2.csv\").astype(bool)\n",
    "            self.molecule_names = self.molecule_names[self.training_mask]\n",
    "            \n",
    "    def len(self) -> int:\n",
    "        return len(self.molecule_names)\n",
    "    def standarize(self):\n",
    "        mms = MinMaxScaler([-4, 4])\n",
    "        self.charges[\"mulliken_charge\"] = mms.fit_transform(self.charges[[\"mulliken_charge\"]]).squeeze()\n",
    "        self.magnetic_shieldings.iloc[:, 2:] = mms.fit_transform(self.magnetic_shieldings.iloc[:, 2:])\n",
    "        self.dipole_moments.iloc[:, 1:] = mms.fit_transform(self.dipole_moments.iloc[:, 1:])\n",
    "        self.potential_energy[\"potential_energy\"] = mms.fit_transform(self.potential_energy[[\"potential_energy\"]]).squeeze()\n",
    "\n",
    "    def preprocess(self, k = None):\n",
    "        charges = self.charges\n",
    "        magnetic_shieldings = self.magnetic_shieldings\n",
    "        dipole_moments = self.dipole_moments\n",
    "        potential_energy = self.potential_energy\n",
    "        molecule_names = self.molecule_names\n",
    "        target = self.target\n",
    "        structures = self.structures\n",
    "        dfs = [charges, magnetic_shieldings, dipole_moments, potential_energy, target, structures]\n",
    "        for i in range(len(dfs)):\n",
    "            dfs[i] = dfs[i].set_index(\"molecule_name\", drop=True)\n",
    "        charges, magnetic_shieldings, dipole_moments, potential_energy, target, structures = dfs\n",
    "        atoms = structures[\"atom\"].unique()\n",
    "        atoms_id = {atoms[i]:i for i in range(len(atoms))}\n",
    "        training_mask = []\n",
    "        for x, name in enumerate(list(molecule_names)):\n",
    "            any_training_edges = True\n",
    "            coords = structures.loc[name][[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "            n_nodes = coords.shape[0]\n",
    "            print(\"{}/{}\".format(x + 1, len(molecule_names)), end = \"\\r\")\n",
    "            # adj_mat\n",
    "            with open(\"./processed/{}_adj_mat.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, get_distance_matrix(coords, k))\n",
    "            atom_types = structures.loc[name][\"atom\"].replace(atoms_id).to_numpy()\n",
    "            atom_onehot = np.zeros([n_nodes, len(atoms)])\n",
    "            atom_onehot[np.arange(0, n_nodes), atom_types] = 1\n",
    "            charge = charges.loc[name][\"mulliken_charge\"].to_numpy()\n",
    "            shieldings = magnetic_shieldings.loc[name].iloc[:, 2:].to_numpy()\n",
    "            node_features = np.concatenate([charge[:, None], shieldings, atom_onehot], axis=1)\n",
    "            with open(\"./processed/{}_node_attr.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, node_features)\n",
    "            try:\n",
    "                edges_target = target.loc[[name]]\n",
    "                training_mask.append(True)\n",
    "            except KeyError:\n",
    "                training_mask.append(False)\n",
    "                any_training_edges = False\n",
    "                \n",
    "            if any_training_edges:\n",
    "                edges_target[\"type\"] = edges_target[\"type\"].replace(edge_to_int).astype(np.int64)\n",
    "                scalar_coupling = edges_target.loc[:, [\"atom_index_0\", \"atom_index_1\",\"type\",\"scalar_coupling_constant\"]].to_numpy()\n",
    "            else:\n",
    "                scalar_coupling = np.array([-1, -1, -1, 0])\n",
    "            with open(\"./processed/{}_target.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, scalar_coupling)\n",
    "            # Graph features\n",
    "            dipole_moment = dipole_moments.loc[name]\n",
    "            norm_dipole = np.array([np.linalg.norm(dipole_moment)])\n",
    "            potential = potential_energy.loc[name]\n",
    "            graph_features = (np.concatenate([dipole_moment, norm_dipole, potential, np.array([n_nodes])]))\n",
    "            with open(\"./processed/{}_graph_feautures.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, graph_features)\n",
    "            # edge_features\n",
    "            edgelist, edgeattr = get_edge_features(coords, dipole_moment)\n",
    "            with open(\"./processed/{}_edge_list.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, edgelist)\n",
    "            with open(\"./processed/{}_edge_attr.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, edgeattr)\n",
    "            # # metaedge\n",
    "            # incident_edges = edgelist[edgeattr[:,1] == 1]\n",
    "            # metaedge_list, metaedge_attr = get_metaedge_features(incident_edges, coords)\n",
    "            # with open(\"./processed/{}_metaedge_list.csv\".format(name), \"wb\") as f:\n",
    "            #     np.savetxt(f,  metaedge_list)\n",
    "            # with open(\"./processed/{}_metaedge_attr.csv\".format(name), \"wb\") as f:\n",
    "            #     np.savetxt(f, metaedge_attr)\n",
    "        with open(\"./raw/training_mask2.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, np.array(training_mask))\n",
    "                \n",
    "    def mem_load(self):\n",
    "        self.mem = {}\n",
    "        for i, molecule in enumerate(self.molecule_names):\n",
    "            graph_features = pd.read_csv(\"./processed/{}_graph_feautures.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            node_features = pd.read_csv(\"./processed/{}_node_attr.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            atomtypes = node_features[:,-5:].argmax(axis=1)\n",
    "            prop_atoms = props[atomtypes,:]\n",
    "            n_nodes = node_features.shape[0]\n",
    "            graph_features = np.tile(graph_features, [1, n_nodes]).T\n",
    "            node_features = np.concatenate([prop_atoms, node_features, graph_features], axis=1)\n",
    "            target =  pd.read_csv(\"./processed/{}_target.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            edge_type = target[:,2]\n",
    "            edge_type = np.concatenate([edge_type, edge_type], axis=0)\n",
    "            edges_target = target[:,0:2]\n",
    "            target = target[:,3]\n",
    "            target = np.concatenate([target, target])\n",
    "            edges_target = np.concatenate([edges_target, edges_target[:,::-1]], axis=0)\n",
    "            edge_list = pd.read_csv(\"./processed/{}_edge_list.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            edge_attr = pd.read_csv(\"./processed/{}_edge_attr.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            metaedge_list = pd.read_csv(\"./processed/{}_metaedge_list.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            metaedge_attr = pd.read_csv(\"./processed/{}_metaedge_attr.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            data = Data(x=torch.Tensor(node_features), edge_index = torch.Tensor(edge_list).T, y=torch.Tensor(target), edge_attr = torch.Tensor(edge_attr))\n",
    "            data.nodes_target = torch.Tensor(edges_target)\n",
    "            data.nodes = n_nodes\n",
    "            data.edges = edge_list.shape[0]\n",
    "            data.types = torch.Tensor(edge_type)\n",
    "            data.metaedge_list = metaedge_list\n",
    "            data.metaedge_list = metaedge_attr\n",
    "            # data.edge_cross = edgelist\n",
    "            # data.nodes = node_features.shape[0]\n",
    "            self.mem[molecule] = data\n",
    "            print(\"{}/{}\".format(i, len(self.molecule_names)), end = \"\\r\")\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        molecule = self.molecule_names[idx]\n",
    "        return self.mem[molecule]\n",
    "        \n",
    "\n",
    "def get_distance_matrix(X, k=None):\n",
    "    dist = squareform(pdist(X))\n",
    "    if k is not None:\n",
    "        non_k = dist.argsort(axis=1)[:, k+1:]\n",
    "        dist[np.arange(0, dist.shape[0])[:,None], non_k] = 0\n",
    "    return dist\n",
    "\n",
    "def to_batch(list_graphs):\n",
    "    n_nodes = 0\n",
    "    for graph in list_graphs:\n",
    "        graph[\"nodes_target\"] += n_nodes\n",
    "        n_nodes += graph.nodes\n",
    "    return Batch.from_data_list(list_graphs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e2a3228-c249-4a99-9a85-e8b34964c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./train_dataloader.pkl\", \"rb\") as f:\n",
    "    train_dataloader = pickle.load(f)\n",
    "with open(\"./test_dataloader.pkl\", \"rb\") as f:\n",
    "    test_dataloader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e9f941-3bb4-46cd-b74c-fa0efc63b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, num_node_features, dim_embedding, num_graph_features, dim_fc, num_hidden_fc, p_dropout = 0.4):\n",
    "#         super().__init__()\n",
    "#         self.p_dropout = p_dropout\n",
    "#         self.conv1 = GCNConv(num_node_features, dim_embedding)\n",
    "#         self.conv2 = GCNConv(dim_embedding, dim_embedding)\n",
    "#         self.conv3 = GCNConv(dim_embedding, dim_embedding)\n",
    "#         self.conv4 = GCNConv(dim_embedding, dim_embedding)\n",
    "#         self.fc1 = nn.Sequential(nn.Linear(dim_embedding*2 + num_graph_features + 3, num_hidden_fc),\n",
    "#                                nn.ReLU(),\n",
    "#                                nn.BatchNorm1d(num_hidden_fc),\n",
    "#                                nn.Linear(num_hidden_fc, dim_fc))\n",
    "#         self.fc2 = (nn.Sequential(nn.Linear(dim_fc, num_hidden_fc),\n",
    "#                                nn.ReLU(),\n",
    "#                                nn.Dropout(p=p_dropout),\n",
    "#                                nn.Linear(num_hidden_fc, dim_fc)))\n",
    "#         self.regressor = (nn.Sequential(nn.Linear(dim_fc, 64),\n",
    "#                                nn.ReLU(),\n",
    "#                                nn.BatchNorm1d(64),\n",
    "#                                nn.Linear(64, 16),\n",
    "#                                nn.ReLU(),\n",
    "#                                nn.Linear(16, 1)))\n",
    "#         self.conv1.apply(init_weights)\n",
    "#         self.conv2.apply(init_weights)\n",
    "#         self.conv3.apply(init_weights)\n",
    "#         self.conv4.apply(init_weights)\n",
    "#         self.fc1.apply(init_weights)\n",
    "#         self.fc2.apply(init_weights)\n",
    "#         self.regressor.apply(init_weights)\n",
    "#     def forward(self, x, adj_mat, graph_features, edge_type, edge_mask):\n",
    "#         n_nodes = adj_mat.shape[0]\n",
    "#         n_edges = edge_mask.sum().item()\n",
    "#         nodes_in_edge= np.array([x for x in itertools.product(range(n_nodes), range(n_nodes))])[edge_mask.cpu().numpy()]\n",
    "#         x = self.conv1(x, adj_mat)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.dropout(x, training=self.training, p=self.p_dropout)\n",
    "#         x = self.conv2(x, adj_mat)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.dropout(x, training=self.training, p=self.p_dropout)\n",
    "#         x = self.conv3(x, adj_mat)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.dropout(x, training=self.training, p=self.p_dropout)\n",
    "#         x = self.conv4(x, adj_mat)\n",
    "#         f_n1 = x[0, nodes_in_edge[:, 0]]\n",
    "#         f_n2 = x[0, nodes_in_edge[:, 1]]\n",
    "#         x = torch.concat([f_n1, f_n2], axis=1)\n",
    "#         gf_rep = graph_features.unsqueeze(0).expand((int(n_edges), 4))\n",
    "#         x = torch.concat([x, gf_rep, edge_type[edge_mask]], axis=1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x) + x\n",
    "#         x = self.regressor(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c4a9cf-0e2a-48fa-b2a3-1085e0dafbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConvEncoderGated(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_features, heads, n_layers, p_dropout):\n",
    "        super().__init__()\n",
    "        self.p_dropout = p_dropout\n",
    "        assert n_layers > 1\n",
    "        self.init_conv = TransformerConv(num_node_features, hidden_features, heads=n_heads, dropout=p_dropout,  edge_dim=8, concat=False)\n",
    "        self.layers = nn.ModuleList([TransformerConv(hidden_features, hidden_features, heads=n_heads, dropout=p_dropout,  edge_dim=8, concat=False) for i in range(n_layers)])\n",
    "        self.gates = nn.Parameter(torch.Tensor(n_layers))\n",
    "        self.init_conv.apply(init_weights)\n",
    "        for conv in self.layers:\n",
    "            conv.apply(init_weights)\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        range_gates = torch.sigmoid(self.gates)\n",
    "        x = self.init_conv(x, edge_index, edge_attr)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.leaky_relu(x)\n",
    "            x = (range_gates[i])*layer(x, edge_index, edge_attr) + (1-range_gates[i])*x\n",
    "        return x\n",
    "    \n",
    "class ResNetGated(nn.Module):\n",
    "    def __init__(self, init_dim, hidden_dim, layers, p_dropout):\n",
    "        super().__init__()\n",
    "        self.p_dropout = p_dropout\n",
    "        assert n_layers > 1\n",
    "        self.layers = nn.ModuleList([nn.Sequential(nn.Linear(init_dim, hidden_dim),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=p_dropout),\n",
    "                             nn.Linear( hidden_dim, init_dim)) for i in range(layers)])\n",
    "        self.gates = nn.Parameter(torch.Tensor(n_layers))\n",
    "        self.layers.apply(init_weights)\n",
    "    def forward(self, x):\n",
    "        range_gates = torch.sigmoid(self.gates)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(x)\n",
    "            x = (range_gates[i])*layer(x) + (1-range_gates[i])*x\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, out_features, n_heads, n_layers, n_res, p_dropout):\n",
    "        super().__init__()\n",
    "        self.conv = TransformerConvEncoderGated(num_node_features, out_features, heads=n_heads, p_dropout=p_dropout,  n_layers=3)\n",
    "        self.fcs = nn.ModuleList([nn.Sequential(ResNetGated(out_features*2, out_features*64, n_res, p_dropout),\n",
    "                               nn.Linear(2 * out_features, 1)) for i in range(8)])\n",
    "        for fc in self.fcs:\n",
    "            fc.apply(init_weights)\n",
    "    def forward(self, x, edge_index, edge_attr, edge_cross, types):\n",
    "        x = self.conv(x, edge_index, edge_attr)\n",
    "        x = x[edge_cross]\n",
    "        shp = x.shape\n",
    "        x = x.transpose(1, 2).reshape([shp[0], shp[2]*2])\n",
    "        xs = []\n",
    "        for i in range(8):\n",
    "            xs.append(self.fcs[i](x[types == i]))\n",
    "        x = torch.concat(xs, axis=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ce230f5-ad68-46ae-9dda-d7717bf12ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv): TransformerConvEncoderGated(\n",
       "    (init_conv): TransformerConv(29, 128, heads=6)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerConv(128, 128, heads=6)\n",
       "      (1): TransformerConv(128, 128, heads=6)\n",
       "      (2): TransformerConv(128, 128, heads=6)\n",
       "    )\n",
       "  )\n",
       "  (fcs): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define the loss function\n",
    "loss_fn = nn.MSELoss\n",
    "\n",
    "lr= 0.001\n",
    "weight_decay = 0.000001\n",
    "p_dropout = 0.001\n",
    "conv_features = 128\n",
    "n_heads = 6\n",
    "n_layers = 3\n",
    "n_res = 2\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "gcn = GCN(29, conv_features, n_heads, n_layers, n_res, p_dropout=p_dropout)\n",
    "params_to_optimize = [\n",
    "    {'params': gcn.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=weight_decay)\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "gcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d37264b3-fab5-459d-ac0c-80187b32c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "RUN = date + \"_lr={}_wd={}_p={}_conv_features={}_n_layers={}_n_res={}\".format(lr,\n",
    "                                                            weight_decay,\n",
    "                                                            p_dropout,\n",
    "                                                            conv_features,\n",
    "                                                             n_layers,\n",
    "                                                            n_res\n",
    "                                                            )\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"./runs/{}\".format(RUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63ada3f4-36ba-41d6-bdca-6966e538843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, data ,loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(data):\n",
    "        x, edge_index, edge_attr, target, edge_cross, types = (batch[\"x\"],\n",
    "                                                               batch[\"edge_index\"],\n",
    "                                                               batch[\"edge_attr\"],\n",
    "                                                               batch[\"y\"],\n",
    "                                                               batch[\"nodes_target\"],\n",
    "                                                               batch[\"types\"])\n",
    "        types_cpu = types.numpy()\n",
    "        sort_index = torch.Tensor(types.numpy().argsort(kind=\"stable\")).long()\n",
    "        target = target[sort_index]\n",
    "        x, edge_index, edge_attr, target, edge_cross, types = x.to(device), \\\n",
    "                                                            edge_index.long().to(device), \\\n",
    "                                                            edge_attr.to(device), \\\n",
    "                                                            target.to(device),\\\n",
    "                                                            edge_cross.long().to(device), \\\n",
    "                                                            types.long().to(device)\n",
    "        logits = model(x, edge_index, edge_attr, edge_cross, types)\n",
    "        loss = loss_fn()\n",
    "        output=loss(logits.squeeze(), target.squeeze())\n",
    "        output.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "        optimizer.step()\n",
    "        train_loss = output.data.cpu().numpy()\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "    return np.mean(train_losses)\n",
    "\n",
    "### Testing function\n",
    "def test(model, device, data, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for i, batch in enumerate(data):\n",
    "            x, edge_index, edge_attr, target, edge_cross, types = (batch[\"x\"],\n",
    "                                                               batch[\"edge_index\"],\n",
    "                                                               batch[\"edge_attr\"],\n",
    "                                                               batch[\"y\"],\n",
    "                                                               batch[\"nodes_target\"],\n",
    "                                                               batch[\"types\"])\n",
    "            types_cpu = types.numpy()\n",
    "            sort_index = torch.Tensor(types.numpy().argsort(kind=\"stable\")).long()\n",
    "            target = target[sort_index]\n",
    "            x, edge_index, edge_attr, target, edge_cross, types = x.to(device), \\\n",
    "                                                                edge_index.long().to(device), \\\n",
    "                                                                edge_attr.to(device), \\\n",
    "                                                                target.to(device),\\\n",
    "                                                                edge_cross.long().to(device), \\\n",
    "                                                                types.long().to(device)\n",
    "            logits = model(x, edge_index, edge_attr, edge_cross, types)\n",
    "            loss = loss_fn()\n",
    "            output=loss(logits.squeeze(), target.squeeze())\n",
    "            test_loss = output.data.cpu().numpy()\n",
    "            test_losses.append(test_loss)\n",
    "    return np.mean(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dcf76ea-d1f1-4b37-a45f-85cfeaf74b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EPOCH 1/100 \t train loss 187.9234619140625 \t \t val loss 32.71564483642578\n",
      "\n",
      " EPOCH 2/100 \t train loss 16.286441802978516 \t \t val loss 12.039107322692871\n",
      "\n",
      " EPOCH 3/100 \t train loss 10.107534408569336 \t \t val loss 7.627233505249023\n",
      "\n",
      " EPOCH 4/100 \t train loss 8.221412658691406 \t \t val loss 6.268552780151367\n",
      "\n",
      " EPOCH 5/100 \t train loss 6.287989616394043 \t \t val loss 4.871088981628418\n",
      "\n",
      " EPOCH 6/100 \t train loss 5.3367204666137695 \t \t val loss 4.292757511138916\n",
      "\n",
      " EPOCH 7/100 \t train loss 4.458630084991455 \t \t val loss 3.9814343452453613\n",
      "\n",
      " EPOCH 8/100 \t train loss 3.870039939880371 \t \t val loss 5.026296615600586\n",
      "\n",
      " EPOCH 9/100 \t train loss 3.697369337081909 \t \t val loss 3.3070297241210938\n",
      "\n",
      " EPOCH 10/100 \t train loss 3.264415979385376 \t \t val loss 2.8673670291900635\n",
      "\n",
      " EPOCH 11/100 \t train loss 2.928161144256592 \t \t val loss 3.7500627040863037\n",
      "\n",
      " EPOCH 12/100 \t train loss 2.6872169971466064 \t \t val loss 2.274897575378418\n",
      "\n",
      " EPOCH 13/100 \t train loss 2.415036916732788 \t \t val loss 2.6823036670684814\n",
      "\n",
      " EPOCH 14/100 \t train loss 2.3185009956359863 \t \t val loss 2.2524704933166504\n",
      "\n",
      " EPOCH 15/100 \t train loss 2.0374884605407715 \t \t val loss 2.054701566696167\n",
      "\n",
      " EPOCH 16/100 \t train loss 2.050328493118286 \t \t val loss 2.826772689819336\n",
      "\n",
      " EPOCH 17/100 \t train loss 1.8290798664093018 \t \t val loss 1.8834081888198853\n",
      "\n",
      " EPOCH 18/100 \t train loss 1.7833292484283447 \t \t val loss 1.8525980710983276\n",
      "\n",
      " EPOCH 19/100 \t train loss 1.7110252380371094 \t \t val loss 1.905394196510315\n",
      "\n",
      " EPOCH 20/100 \t train loss 1.6284233331680298 \t \t val loss 1.5588231086730957\n",
      "\n",
      " EPOCH 21/100 \t train loss 1.5444256067276 \t \t val loss 1.3954850435256958\n",
      "\n",
      " EPOCH 22/100 \t train loss 1.5140135288238525 \t \t val loss 1.5437250137329102\n",
      "\n",
      " EPOCH 23/100 \t train loss 1.429046630859375 \t \t val loss 1.4202104806900024\n",
      "\n",
      " EPOCH 24/100 \t train loss 1.3718128204345703 \t \t val loss 1.569639801979065\n",
      "\n",
      " EPOCH 25/100 \t train loss 1.3143523931503296 \t \t val loss 2.0128936767578125\n",
      "\n",
      " EPOCH 26/100 \t train loss 1.2798126935958862 \t \t val loss 1.3898565769195557\n",
      "\n",
      " EPOCH 27/100 \t train loss 1.2463172674179077 \t \t val loss 1.4124127626419067\n",
      "\n",
      " EPOCH 28/100 \t train loss 1.1737762689590454 \t \t val loss 1.2010221481323242\n",
      "\n",
      " EPOCH 29/100 \t train loss 1.146198034286499 \t \t val loss 1.2528502941131592\n",
      "\n",
      " EPOCH 30/100 \t train loss 1.1068533658981323 \t \t val loss 1.2977104187011719\n",
      "\n",
      " EPOCH 31/100 \t train loss 1.0977096557617188 \t \t val loss 1.135176181793213\n",
      "\n",
      " EPOCH 32/100 \t train loss 1.060792326927185 \t \t val loss 1.30592942237854\n",
      "\n",
      " EPOCH 33/100 \t train loss 1.039711594581604 \t \t val loss 1.016679286956787\n",
      "\n",
      " EPOCH 34/100 \t train loss 0.9935884475708008 \t \t val loss 1.0400406122207642\n",
      "\n",
      " EPOCH 35/100 \t train loss 0.9713298082351685 \t \t val loss 0.9266621470451355\n",
      "\n",
      " EPOCH 36/100 \t train loss 0.9280368685722351 \t \t val loss 0.9769250750541687\n",
      "\n",
      " EPOCH 37/100 \t train loss 0.9410102367401123 \t \t val loss 0.9669550657272339\n",
      "\n",
      " EPOCH 38/100 \t train loss 0.8959744572639465 \t \t val loss 1.1367688179016113\n",
      "\n",
      " EPOCH 39/100 \t train loss 0.8764364719390869 \t \t val loss 0.890540361404419\n",
      "\n",
      " EPOCH 40/100 \t train loss 0.8587779998779297 \t \t val loss 0.9375678896903992\n",
      "\n",
      " EPOCH 41/100 \t train loss 0.8702749609947205 \t \t val loss 1.015430212020874\n",
      "\n",
      " EPOCH 42/100 \t train loss 0.8025072813034058 \t \t val loss 0.981484591960907\n",
      "\n",
      " EPOCH 43/100 \t train loss 0.8210940957069397 \t \t val loss 0.8555572032928467\n",
      "\n",
      " EPOCH 44/100 \t train loss 0.803846001625061 \t \t val loss 0.8359205722808838\n",
      "\n",
      " EPOCH 45/100 \t train loss 0.7923319339752197 \t \t val loss 0.8170573711395264\n",
      "\n",
      " EPOCH 46/100 \t train loss 0.7553234696388245 \t \t val loss 0.9353715777397156\n",
      "\n",
      " EPOCH 47/100 \t train loss 0.7576872706413269 \t \t val loss 0.8374201655387878\n",
      "\n",
      " EPOCH 48/100 \t train loss 0.7276700139045715 \t \t val loss 0.8807395696640015\n",
      "\n",
      " EPOCH 49/100 \t train loss 0.7365540266036987 \t \t val loss 0.7286145687103271\n",
      "\n",
      " EPOCH 50/100 \t train loss 0.7373429536819458 \t \t val loss 0.8001382946968079\n",
      "\n",
      " EPOCH 51/100 \t train loss 0.7163329124450684 \t \t val loss 0.8402148485183716\n",
      "\n",
      " EPOCH 52/100 \t train loss 0.72273188829422 \t \t val loss 0.7552192807197571\n",
      "\n",
      " EPOCH 53/100 \t train loss 0.7182918190956116 \t \t val loss 0.9724445343017578\n",
      "\n",
      " EPOCH 54/100 \t train loss 0.7133222222328186 \t \t val loss 0.8069391846656799\n",
      "\n",
      " EPOCH 55/100 \t train loss 0.7140559554100037 \t \t val loss 0.7062731981277466\n",
      "\n",
      " EPOCH 56/100 \t train loss 0.665181040763855 \t \t val loss 0.799131453037262\n",
      "\n",
      " EPOCH 57/100 \t train loss 0.68598473072052 \t \t val loss 0.8906020522117615\n",
      "\n",
      " EPOCH 58/100 \t train loss 0.6396312713623047 \t \t val loss 0.8176895380020142\n",
      "\n",
      " EPOCH 59/100 \t train loss 0.6506447792053223 \t \t val loss 0.7762377262115479\n",
      "\n",
      " EPOCH 60/100 \t train loss 0.6487956643104553 \t \t val loss 0.750007688999176\n",
      "\n",
      " EPOCH 61/100 \t train loss 0.6564655900001526 \t \t val loss 0.7992361187934875\n",
      "\n",
      " EPOCH 62/100 \t train loss 0.6435031890869141 \t \t val loss 0.7698103785514832\n",
      "\n",
      " EPOCH 63/100 \t train loss 0.6404597759246826 \t \t val loss 0.7288972735404968\n",
      "\n",
      " EPOCH 64/100 \t train loss 0.638746976852417 \t \t val loss 0.646695077419281\n",
      "\n",
      " EPOCH 65/100 \t train loss 0.6351068019866943 \t \t val loss 0.750310480594635\n",
      "\n",
      " EPOCH 66/100 \t train loss 0.6303687691688538 \t \t val loss 0.6677181720733643\n",
      "\n",
      " EPOCH 67/100 \t train loss 0.6200141310691833 \t \t val loss 0.9023315906524658\n",
      "\n",
      " EPOCH 68/100 \t train loss 0.6118556261062622 \t \t val loss 0.7598379254341125\n",
      "\n",
      " EPOCH 69/100 \t train loss 0.6121615171432495 \t \t val loss 0.7912548780441284\n",
      "\n",
      " EPOCH 70/100 \t train loss 0.6156153082847595 \t \t val loss 0.8126846551895142\n",
      "\n",
      " EPOCH 71/100 \t train loss 0.6058491468429565 \t \t val loss 0.6514403223991394\n",
      "\n",
      " EPOCH 72/100 \t train loss 0.5836864709854126 \t \t val loss 0.7307985424995422\n",
      "\n",
      " EPOCH 73/100 \t train loss 0.6108421683311462 \t \t val loss 0.7157084941864014\n",
      "\n",
      " EPOCH 74/100 \t train loss 0.5941754579544067 \t \t val loss 0.7214563488960266\n",
      "\n",
      " EPOCH 75/100 \t train loss 0.5726703405380249 \t \t val loss 0.7518030405044556\n",
      "\n",
      " EPOCH 76/100 \t train loss 0.5945137143135071 \t \t val loss 0.7310390472412109\n",
      "\n",
      " EPOCH 77/100 \t train loss 0.5778366923332214 \t \t val loss 0.6636046171188354\n",
      "\n",
      " EPOCH 78/100 \t train loss 0.5596520900726318 \t \t val loss 0.6586434245109558\n",
      "\n",
      " EPOCH 79/100 \t train loss 0.5717243552207947 \t \t val loss 0.6328286528587341\n",
      "\n",
      " EPOCH 80/100 \t train loss 0.557178795337677 \t \t val loss 0.6598745584487915\n",
      "\n",
      " EPOCH 81/100 \t train loss 0.5780226588249207 \t \t val loss 0.6686148047447205\n",
      "\n",
      " EPOCH 82/100 \t train loss 0.5628912448883057 \t \t val loss 0.6723296642303467\n",
      "\n",
      " EPOCH 83/100 \t train loss 0.5137807726860046 \t \t val loss 0.5533221364021301\n",
      "\n",
      " EPOCH 84/100 \t train loss 0.5737255215644836 \t \t val loss 0.5820318460464478\n",
      "\n",
      " EPOCH 85/100 \t train loss 0.5569775700569153 \t \t val loss 0.6725540161132812\n",
      "\n",
      " EPOCH 86/100 \t train loss 0.5538513660430908 \t \t val loss 0.6592910289764404\n",
      "\n",
      " EPOCH 87/100 \t train loss 0.5270565152168274 \t \t val loss 0.6066339015960693\n",
      "\n",
      " EPOCH 88/100 \t train loss 0.5288138389587402 \t \t val loss 0.7111380696296692\n",
      "\n",
      " EPOCH 89/100 \t train loss 0.5316991806030273 \t \t val loss 0.6872372627258301\n",
      "\n",
      " EPOCH 90/100 \t train loss 0.528445303440094 \t \t val loss 0.722163736820221\n",
      "\n",
      " EPOCH 91/100 \t train loss 0.5549407005310059 \t \t val loss 0.5509958863258362\n",
      "\n",
      " EPOCH 92/100 \t train loss 0.5125865936279297 \t \t val loss 0.6438550353050232\n",
      "\n",
      " EPOCH 93/100 \t train loss 0.5503435730934143 \t \t val loss 0.48794662952423096\n",
      "\n",
      " EPOCH 94/100 \t train loss 0.5079576969146729 \t \t val loss 0.5679348707199097\n",
      "\n",
      " EPOCH 95/100 \t train loss 0.5339480042457581 \t \t val loss 0.6285159587860107\n",
      "\n",
      " EPOCH 96/100 \t train loss 0.5173080563545227 \t \t val loss 0.7426786422729492\n",
      "\n",
      " EPOCH 97/100 \t train loss 0.5387561917304993 \t \t val loss 0.6614763736724854\n",
      "\n",
      " EPOCH 98/100 \t train loss 0.48429059982299805 \t \t val loss 0.5062455534934998\n",
      "\n",
      " EPOCH 99/100 \t train loss 0.5206488966941833 \t \t val loss 0.5085024833679199\n",
      "\n",
      " EPOCH 100/100 \t train loss 0.5132678747177124 \t \t val loss 0.6904162168502808\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1000\n",
    "num_epochs = 100\n",
    "diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(gcn, device, train_dataloader, loss_fn, optim)\n",
    "    test_loss = test(gcn, device, test_dataloader, loss_fn)\n",
    "    print('\\n EPOCH {}/{} \\t train loss {} \\t \\t val loss {}'.format(epoch + 1, num_epochs, train_loss, test_loss))\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(test_loss)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(gcn.state_dict(), \"./saved_models/with_resnet_{}.pth\".format(RUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cdc9de2-1a6b-4df9-820e-edbee6326cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-02-14:46:07_lr=0.001_wd=1e-06_p=0.001_conv_features=128_n_layers=3_n_res=2\n"
     ]
    }
   ],
   "source": [
    "print(RUN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi_python",
   "language": "python",
   "name": "mpi_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
