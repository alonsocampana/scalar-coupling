{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fd615a-450f-46f1-ab39-b6fcef802df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url, Data, Batch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mendeleev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e36dfec-0763-4e55-be3a-e90b5ecd89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mendeleev import element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7e0a8f-6d98-4bf3-9295-b423e279dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, H, N, O, F = element([\"C\", \"H\", \"N\", \"O\", \"F\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b06396-df25-4d9b-bd0b-476c3107033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for e in [C, H, N, O, F]:\n",
    "    row = [e.atomic_radius,\n",
    "           e.atomic_volume,\n",
    "           e.atomic_weight,\n",
    "           e.boiling_point,\n",
    "           e.covalent_radius_bragg,\n",
    "           e.dipole_polarizability,\n",
    "           e.electron_affinity,\n",
    "           e.en_ghosh,\n",
    "           e.vdw_radius]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893b4d2d-655b-435e-b955-c06190188fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties=np.array(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d88aec5-a3e5-40f4-85f9-e4246345bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties[properties == None] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9022d475-1227-4fcf-9f82-f49b6174f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "emms = MinMaxScaler([-5, 5])\n",
    "props = properties.astype(np.float32)\n",
    "props = emms.fit_transform(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11280e70-1350-4fdc-b5ea-594205942b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_features(coords, dipole_moment, eps = 0.000001):\n",
    "    norm_dipole = np.linalg.norm(dipole_moment) # get the norm to normalize the vector and find angles\n",
    "    distmat = squareform(pdist(coords)) # get dist_mat to find edges\n",
    "    np.fill_diagonal(distmat, np.nan) # fill to avoid ranking problem\n",
    "    rankings = distmat.argsort(axis=1).argsort(axis=1) # order distance matrix to get n-neighborhood of each edge\n",
    "    G = nx.from_numpy_matrix(distmat, create_using=nx.DiGraph())\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) # remove to avoid self edges\n",
    "    edgelist = nx.to_edgelist(G)\n",
    "    edgelist, edge_features = edge_list_to_numpy(edgelist) #get edges and distances\n",
    "    G = nx.from_numpy_matrix(rankings+1, create_using=nx.DiGraph())\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    edgelist2 = nx.to_edgelist(G)\n",
    "    edgelist2, edge_rankings = edge_list_to_numpy(edgelist2) # get rankings\n",
    "    edge_rankings = edge_rankings - 1\n",
    "    n_edges = edgelist.shape[0]\n",
    "    coords_nodes = coords[edgelist] #select nodes in edges\n",
    "    vectors_edges = coords_nodes.transpose(0, 2, 1)[:, :, 1] - coords_nodes.transpose(0, 2, 1)[:, :, 0] # get vector of each edge\n",
    "    vectors_edges_normalized = vectors_edges/(np.linalg.norm(vectors_edges, axis=1) + eps)[:,None] \n",
    "    dipole_moment_normalized = dipole_moment/(norm_dipole + eps)\n",
    "    angles_dipole_moment = np.dot(vectors_edges_normalized, dipole_moment) # do dot product to find angles\n",
    "    ranks = np.zeros([n_edges, 6])\n",
    "    edge_rankings[edge_rankings > 4] = 5 # replace to impose same dimensionality in all molecules\n",
    "    ranks[np.arange(n_edges), edge_rankings] = 1\n",
    "    edge_features = np.concatenate([edge_features[:,None], ranks, angles_dipole_moment[:,None]], axis=1) # concatenate all features\n",
    "    return edgelist, edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10999a66-5f37-4f67-bf94-4b9b1ea21329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_list_to_numpy(edgelist):\n",
    "    tail = []\n",
    "    head = []\n",
    "    weight = []\n",
    "    for edge in list(edgelist):\n",
    "        tail.append(edge[0])\n",
    "        head.append(edge[1])\n",
    "        weight.append(edge[2][\"weight\"])\n",
    "    tail = np.array(tail)[:,None]\n",
    "    head = np.array(head)[:,None]\n",
    "    weight = np.array(weight)\n",
    "    return np.concatenate([tail, head], axis=1), weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce54f832-c04b-46b1-b7de-e8e123d761c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fefd4fb-b9cc-43f7-b411-2ad0017f6a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 682, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 887, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/engine/default.py\", line 667, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139799173416768 and this is thread id 139799094974208.\n",
      "IOStream.flush timed out\n",
      "Exception closing connection <sqlite3.Connection object at 0x7f23d7b58210>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 682, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 887, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/engine/default.py\", line 667, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139799173416768 and this is thread id 139799094974208.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 244, in _close_connection\n",
      "    self._dialect.do_close(connection)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/engine/default.py\", line 673, in do_close\n",
      "    dbapi_connection.close()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139799173416768 and this is thread id 139799094974208.\n",
      "IOStream.flush timed out\n",
      "Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 682, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 887, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/engine/default.py\", line 667, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139799173416768 and this is thread id 139799094974208.\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "Exception closing connection <sqlite3.Connection object at 0x7f25806335d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 682, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 887, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/engine/default.py\", line 667, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139799173416768 and this is thread id 139799094974208.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/pool/base.py\", line 244, in _close_connection\n",
      "    self._dialect.do_close(connection)\n",
      "  File \"/home/pedro/anaconda3/envs/mpi/lib/python3.9/site-packages/sqlalchemy/engine/default.py\", line 673, in do_close\n",
      "    dbapi_connection.close()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 139799173416768 and this is thread id 139799094974208.\n",
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "edges = target[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae7f5252-fc4d-47fb-92c0-7104debc580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "edge_to_int = {edges[i]:i for i in range(len(edges))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10764aa2-7d94-479d-b435-2b2625c90271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dihedral(coords, indices):\n",
    "    a = coords[indices[0],:] - coords[indices[1],:]\n",
    "    b = coords[indices[2],:] - coords[indices[1],:]\n",
    "    return a@b/(np.linalg.norm(b) * np.linalg.norm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1629b297-4240-4bf6-9d19-c6e7c60a01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, training= True):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.filenames = pd.read_csv(\"raw/processed_names.csv\")\n",
    "        self.charges = pd.read_csv(\"raw/mulliken_charges.csv\")\n",
    "        self.magnetic_shieldings = pd.read_csv(\"raw/magnetic_shielding_tensors.csv\")\n",
    "        self.dipole_moments = pd.read_csv(\"raw/dipole_moments.csv\")\n",
    "        self.potential_energy = pd.read_csv(\"raw/potential_energy.csv\")\n",
    "        self.target = pd.read_csv(\"raw/train.csv\")\n",
    "        self.structures = pd.read_csv(\"raw/structures.csv\")\n",
    "        self.molecule_names = molecule_names = np.unique(self.potential_energy[\"molecule_name\"])\n",
    "        if training:\n",
    "            self.training_mask = np.loadtxt(\"./raw/training_mask2.csv\").astype(bool)\n",
    "            self.molecule_names = self.molecule_names[self.training_mask]\n",
    "            \n",
    "    def len(self) -> int:\n",
    "        return len(self.molecule_names)\n",
    "    def standarize(self):\n",
    "        mms = MinMaxScaler([-4, 4])\n",
    "        self.charges[\"mulliken_charge\"] = mms.fit_transform(self.charges[[\"mulliken_charge\"]]).squeeze()\n",
    "        self.magnetic_shieldings.iloc[:, 2:] = mms.fit_transform(self.magnetic_shieldings.iloc[:, 2:])\n",
    "        self.dipole_moments.iloc[:, 1:] = mms.fit_transform(self.dipole_moments.iloc[:, 1:])\n",
    "        self.potential_energy[\"potential_energy\"] = mms.fit_transform(self.potential_energy[[\"potential_energy\"]]).squeeze()\n",
    "\n",
    "    def preprocess(self, k = None):\n",
    "        charges = self.charges\n",
    "        magnetic_shieldings = self.magnetic_shieldings\n",
    "        dipole_moments = self.dipole_moments\n",
    "        potential_energy = self.potential_energy\n",
    "        molecule_names = self.molecule_names\n",
    "        target = self.target\n",
    "        structures = self.structures\n",
    "        dfs = [charges, magnetic_shieldings, dipole_moments, potential_energy, target, structures]\n",
    "        for i in range(len(dfs)):\n",
    "            dfs[i] = dfs[i].set_index(\"molecule_name\", drop=True)\n",
    "        charges, magnetic_shieldings, dipole_moments, potential_energy, target, structures = dfs\n",
    "        atoms = structures[\"atom\"].unique()\n",
    "        atoms_id = {atoms[i]:i for i in range(len(atoms))}\n",
    "        training_mask = []\n",
    "        for x, name in enumerate(list(molecule_names)):\n",
    "            any_training_edges = True\n",
    "            coords = structures.loc[name][[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "            n_nodes = coords.shape[0]\n",
    "            print(\"{}/{}\".format(x + 1, len(molecule_names)), end = \"\\r\")\n",
    "            # adj_mat\n",
    "            atom_types = structures.loc[name][\"atom\"].replace(atoms_id).to_numpy()\n",
    "            atom_onehot = np.zeros([n_nodes, len(atoms)])\n",
    "            atom_onehot[np.arange(0, n_nodes), atom_types] = 1\n",
    "            charge = charges.loc[name][\"mulliken_charge\"].to_numpy()\n",
    "            shieldings = magnetic_shieldings.loc[name].iloc[:, 2:].to_numpy()\n",
    "            node_features = np.concatenate([charge[:, None], shieldings, atom_onehot, coords], axis=1)\n",
    "            with open(\"./processed2/{}_node_attr.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, node_features)\n",
    "            try:\n",
    "                edges_target = target.loc[[name]]\n",
    "                training_mask.append(True)\n",
    "            except KeyError:\n",
    "                training_mask.append(False)\n",
    "                any_training_edges = False\n",
    "                \n",
    "            if any_training_edges:\n",
    "                edges_target[\"type\"] = edges_target[\"type\"].replace(edge_to_int).astype(np.int64)\n",
    "                scalar_coupling = edges_target.loc[:, [\"atom_index_0\", \"atom_index_1\",\"type\",\"scalar_coupling_constant\"]].to_numpy()\n",
    "            else:\n",
    "                scalar_coupling = np.array([-1, -1, -1, 0])\n",
    "            with open(\"./processed2/{}_target.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, scalar_coupling)\n",
    "            # Graph features\n",
    "            dipole_moment = dipole_moments.loc[name]\n",
    "            norm_dipole = np.array([np.linalg.norm(dipole_moment)])\n",
    "            potential = potential_energy.loc[name]\n",
    "            graph_features = (np.concatenate([dipole_moment, norm_dipole, potential, np.array([n_nodes]), atom_onehot.sum(axis=0)]))\n",
    "            with open(\"./processed2/{}_graph_features.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, graph_features)\n",
    "            # edge_features\n",
    "            edgelist, edge_attr = get_edge_features(coords, dipole_moment)\n",
    "            # metaedge\n",
    "            second_nhood = edgelist[edge_attr[:, 2] == 1]\n",
    "            second_nhood = np.unique(np.sort(second_nhood, axis=1), axis=0)\n",
    "            first_nhood = edgelist[edge_attr[:, 1] == 1]\n",
    "            first_nhood = np.unique(np.sort(first_nhood, axis=1), axis=0)\n",
    "            is_in_first = [x in first_nhood.tolist() for x in second_nhood.tolist()]\n",
    "            second_nhood = second_nhood[~np.array(is_in_first)]\n",
    "            meta_edges = []\n",
    "            normal_edges = []\n",
    "            for i in first_nhood:\n",
    "                for j in first_nhood:\n",
    "                    meta_edge = np.unique(np.concatenate([i, j]))\n",
    "                    if len(meta_edge) == 3:\n",
    "                        for k in second_nhood:\n",
    "                            is_in_meta = np.in1d(k, meta_edge)\n",
    "                            if is_in_meta.all():\n",
    "                                center_node = (np.setdiff1d(meta_edge, k))\n",
    "                                meta_edges.append(np.array([k[0], center_node[0], k[1]]))\n",
    "                                normal_edges.append(k)\n",
    "            if len(meta_edges) > 0:         \n",
    "                unique_dihedral_edges, is_unique = np.unique(np.stack(normal_edges), axis=0, return_index=True)\n",
    "                meta_edges_un = np.stack(meta_edges)[is_unique]\n",
    "                angles = []\n",
    "                for me in meta_edges_un.astype(np.int64):\n",
    "                    angles.append(get_dihedral(coords, me))\n",
    "                dihedral_angles = np.zeros(edgelist.shape[0])\n",
    "                for i in range(len(angles)):\n",
    "                    is_edge = np.where(edgelist == unique_dihedral_edges[i], 1, 0).all(axis=1)\n",
    "                    dihedral_angles[is_edge] = angles[i]\n",
    "                    is_edge = np.where(edgelist == unique_dihedral_edges[i][::-1], 1, 0).all(axis=1)\n",
    "                    dihedral_angles[is_edge] = angles[i]\n",
    "            else:\n",
    "                dihedral_angles = np.zeros(edgelist.shape[0])\n",
    "            with open(\"./processed2/{}_edge_list.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, edgelist)\n",
    "            with open(\"./processed2/{}_edgeattr.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, np.concatenate([edge_attr, dihedral_angles[:,None]], axis=1))\n",
    "                \n",
    "    def mem_load(self):\n",
    "        self.mem = {}\n",
    "        for i, molecule in enumerate(self.molecule_names):\n",
    "            graph_features = pd.read_csv(\"./processed2/{}_graph_features.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            node_features = pd.read_csv(\"./processed2/{}_node_attr.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            atomtypes = node_features[:,-5:].argmax(axis=1)\n",
    "            prop_atoms = props[atomtypes,:]\n",
    "            n_nodes = node_features.shape[0]\n",
    "            graph_features = np.tile(graph_features, [1, n_nodes]).T\n",
    "            node_features = np.concatenate([prop_atoms, node_features, graph_features], axis=1)\n",
    "            target =  pd.read_csv(\"./processed2/{}_target.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            edge_type = target[:,2]\n",
    "            edge_type = np.concatenate([edge_type, edge_type], axis=0)\n",
    "            edges_target = target[:,0:2]\n",
    "            target = target[:,3]\n",
    "            target = np.concatenate([target, target])\n",
    "            edges_target = np.concatenate([edges_target, edges_target[:,::-1]], axis=0)\n",
    "            edge_list = pd.read_csv(\"./processed2/{}_edge_list.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            edge_attr = pd.read_csv(\"./processed2/{}_edgeattr.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            data = Data(x=torch.Tensor(node_features), edge_index = torch.Tensor(edge_list).T, y=torch.Tensor(target), edge_attr = torch.Tensor(edge_attr))\n",
    "            data.nodes_target = torch.Tensor(edges_target)\n",
    "            data.nodes = n_nodes\n",
    "            data.edges = edge_list.shape[0]\n",
    "            data.types = torch.Tensor(edge_type)\n",
    "            # data.edge_cross = edgelist\n",
    "            # data.nodes = node_features.shape[0]\n",
    "            self.mem[molecule] = data\n",
    "            print(\"{}/{}\".format(i, len(self.molecule_names)), end = \"\\r\")\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        molecule = self.molecule_names[idx]\n",
    "        return self.mem[molecule]\n",
    "        \n",
    "\n",
    "def get_distance_matrix(X, k=None):\n",
    "    dist = squareform(pdist(X))\n",
    "    if k is not None:\n",
    "        non_k = dist.argsort(axis=1)[:, k+1:]\n",
    "        dist[np.arange(0, dist.shape[0])[:,None], non_k] = 0\n",
    "    return dist\n",
    "\n",
    "def to_batch(list_graphs):\n",
    "    n_nodes = 0\n",
    "    for graph in list_graphs:\n",
    "        graph[\"nodes_target\"] += n_nodes\n",
    "        n_nodes += graph.nodes\n",
    "    return Batch.from_data_list(list_graphs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d3a178-34b0-480c-8820-16c880832b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SCDataset('/media/pedro/data/projects/scalar_coupling', training=False)\n",
    "dataset.standarize()\n",
    "dataset.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5782013-eab9-47f3-a4d1-9fc938476ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SCDataset('/media/pedro/data/projects/scalar_coupling', training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f66a617f-b029-4079-aafe-a28c3168a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.mem_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07ca7312-3681-4bfb-932b-213996ab6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = int(len(dataset) * 0.9)\n",
    "test_length = len(dataset) - train_length\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_length, test_length])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=128, collate_fn=to_batch, shuffle=True, num_workers=20)\n",
    "test_dataloader = torch.utils.data.DataLoader(val_set, batch_size=128, collate_fn=to_batch, shuffle=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90e9f941-3bb4-46cd-b74c-fa0efc63b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATv2Conv, GATConv, SAGEConv, PDNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c4a9cf-0e2a-48fa-b2a3-1085e0dafbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATv2EncoderGated(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_features, heads, n_layers, p_dropout):\n",
    "        super().__init__()\n",
    "        self.p_dropout = p_dropout\n",
    "        assert n_layers > 1\n",
    "        self.init_conv = GATv2Conv(num_node_features, hidden_features, heads=n_heads, dropout=p_dropout,  edge_dim=9, concat=False)\n",
    "        self.layers = nn.ModuleList([GATv2Conv(hidden_features, hidden_features, heads=n_heads, dropout=p_dropout,  edge_dim=9, concat=False) for i in range(n_layers-1)])\n",
    "        self.gates = nn.Parameter(torch.Tensor(n_layers))\n",
    "        self.init_conv.apply(init_weights)\n",
    "        for conv in self.layers:\n",
    "            conv.apply(init_weights)\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        range_gates = torch.sigmoid(self.gates)\n",
    "        x = self.init_conv(x, edge_index, edge_attr)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.leaky_relu(x)\n",
    "            x = (range_gates[i])*layer(x, edge_index, edge_attr) + (1-range_gates[i])*x\n",
    "        return x\n",
    "    \n",
    "class ResNetGated(nn.Module):\n",
    "    def __init__(self, init_dim, hidden_dim, layers, p_dropout):\n",
    "        super().__init__()\n",
    "        self.p_dropout = p_dropout\n",
    "        assert n_layers > 1\n",
    "        self.layers = nn.ModuleList([nn.Sequential(nn.Linear(init_dim, hidden_dim),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=p_dropout),\n",
    "                             nn.Linear( hidden_dim, init_dim)) for i in range(layers)])\n",
    "        self.gates = nn.Parameter(torch.Tensor(n_layers))\n",
    "        self.layers.apply(init_weights)\n",
    "    def forward(self, x):\n",
    "        range_gates = torch.sigmoid(self.gates)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(x)\n",
    "            x = (range_gates[i])*layer(x) + (1-range_gates[i])*x\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, out_features, n_heads, n_layers, n_res, p_dropout):\n",
    "        super().__init__()\n",
    "        self.conv = GATv2EncoderGated(num_node_features, out_features, heads=n_heads, p_dropout=p_dropout,  n_layers=n_layers)\n",
    "        self.fcs = nn.ModuleList([nn.Sequential(ResNetGated(out_features*2, out_features*64, n_res, p_dropout),\n",
    "                               nn.Linear(2 * out_features, 1)) for i in range(8)])\n",
    "        for fc in self.fcs:\n",
    "            fc.apply(init_weights)\n",
    "    def forward(self, x, edge_index, edge_attr, edge_cross, types):\n",
    "        x = self.conv(x, edge_index, edge_attr)\n",
    "        x = x[edge_cross]\n",
    "        shp = x.shape\n",
    "        x = x.transpose(1, 2).reshape([shp[0], shp[2]*2])\n",
    "        xs = []\n",
    "        for i in range(8):\n",
    "            xs.append(self.fcs[i](x[types == i]))\n",
    "        x = torch.concat(xs, axis=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ce230f5-ad68-46ae-9dda-d7717bf12ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv): GATv2EncoderGated(\n",
       "    (init_conv): GATv2Conv(37, 128, heads=3)\n",
       "    (layers): ModuleList(\n",
       "      (0): GATv2Conv(128, 128, heads=3)\n",
       "      (1): GATv2Conv(128, 128, heads=3)\n",
       "    )\n",
       "  )\n",
       "  (fcs): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.0343089, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define the loss function\n",
    "loss_fn = nn.MSELoss\n",
    "\n",
    "lr= 0.00078\n",
    "weight_decay = 1.76755e-08\n",
    "p_dropout = 0.0343089\n",
    "conv_features = 128\n",
    "n_heads = 3\n",
    "n_layers = 3\n",
    "n_res = 3\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "gcn = GCN(37, conv_features, n_heads, n_layers, n_res, p_dropout=p_dropout)\n",
    "params_to_optimize = [\n",
    "    {'params': gcn.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=weight_decay)\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "gcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d37264b3-fab5-459d-ac0c-80187b32c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "RUN = date + \"_lr={}_wd={}_p={}_conv_features={}_n_layers={}_n_res={}\".format(lr,\n",
    "                                                            weight_decay,\n",
    "                                                            p_dropout,\n",
    "                                                            conv_features,\n",
    "                                                             n_layers,\n",
    "                                                            n_res\n",
    "                                                            )\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"./runs/{}\".format(RUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ada3f4-36ba-41d6-bdca-6966e538843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, data ,loss_fn, optimizer, batch_acc=2):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(data):\n",
    "        x, edge_index, edge_attr, target, edge_cross, types = (batch[\"x\"],\n",
    "                                                               batch[\"edge_index\"],\n",
    "                                                               batch[\"edge_attr\"],\n",
    "                                                               batch[\"y\"],\n",
    "                                                               batch[\"nodes_target\"],\n",
    "                                                               batch[\"types\"])\n",
    "        types_cpu = types.numpy()\n",
    "        sort_index = torch.Tensor(types.numpy().argsort(kind=\"stable\")).long()\n",
    "        target = target[sort_index]\n",
    "        x, edge_index, edge_attr, target, edge_cross, types = x.to(device), \\\n",
    "                                                            edge_index.long().to(device), \\\n",
    "                                                            edge_attr.to(device), \\\n",
    "                                                            target.to(device),\\\n",
    "                                                            edge_cross.long().to(device), \\\n",
    "                                                            types.long().to(device)\n",
    "        logits = model(x, edge_index, edge_attr, edge_cross, types)\n",
    "        loss = loss_fn()\n",
    "        output=loss(logits.squeeze(), target.squeeze())\n",
    "        output.backward()\n",
    "        if ((i+1)%batch_acc) == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "        train_loss = output.data.cpu().numpy()\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "    return np.mean(train_losses)\n",
    "\n",
    "### Testing function\n",
    "def test(model, device, data, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for i, batch in enumerate(data):\n",
    "            x, edge_index, edge_attr, target, edge_cross, types = (batch[\"x\"],\n",
    "                                                               batch[\"edge_index\"],\n",
    "                                                               batch[\"edge_attr\"],\n",
    "                                                               batch[\"y\"],\n",
    "                                                               batch[\"nodes_target\"],\n",
    "                                                               batch[\"types\"])\n",
    "            types_cpu = types.numpy()\n",
    "            sort_index = torch.Tensor(types.numpy().argsort(kind=\"stable\")).long()\n",
    "            target = target[sort_index]\n",
    "            x, edge_index, edge_attr, target, edge_cross, types = x.to(device), \\\n",
    "                                                                edge_index.long().to(device), \\\n",
    "                                                                edge_attr.to(device), \\\n",
    "                                                                target.to(device),\\\n",
    "                                                                edge_cross.long().to(device), \\\n",
    "                                                                types.long().to(device)\n",
    "            logits = model(x, edge_index, edge_attr, edge_cross, types)\n",
    "            loss = loss_fn()\n",
    "            output=loss(logits.squeeze(), target.squeeze())\n",
    "            test_loss = output.data.cpu().numpy()\n",
    "            test_losses.append(test_loss)\n",
    "    return np.mean(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dcf76ea-d1f1-4b37-a45f-85cfeaf74b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EPOCH 1/100 \t train loss 354.1801452636719 \t \t val loss 35.28205490112305\n",
      "\n",
      " EPOCH 2/100 \t train loss 29.1060791015625 \t \t val loss 27.516782760620117\n",
      "\n",
      " EPOCH 3/100 \t train loss 21.970190048217773 \t \t val loss 17.692537307739258\n",
      "\n",
      " EPOCH 4/100 \t train loss 18.177515029907227 \t \t val loss 16.3250675201416\n",
      "\n",
      " EPOCH 5/100 \t train loss 13.73880386352539 \t \t val loss 18.16177749633789\n",
      "\n",
      " EPOCH 6/100 \t train loss 11.553083419799805 \t \t val loss 9.14149284362793\n",
      "\n",
      " EPOCH 7/100 \t train loss 9.843188285827637 \t \t val loss 8.704882621765137\n",
      "\n",
      " EPOCH 8/100 \t train loss 7.67933464050293 \t \t val loss 6.186276912689209\n",
      "\n",
      " EPOCH 9/100 \t train loss 6.731311798095703 \t \t val loss 5.988966941833496\n",
      "\n",
      " EPOCH 10/100 \t train loss 5.772371768951416 \t \t val loss 5.999088764190674\n",
      "\n",
      " EPOCH 11/100 \t train loss 5.645722389221191 \t \t val loss 4.658374309539795\n",
      "\n",
      " EPOCH 12/100 \t train loss 4.833165168762207 \t \t val loss 3.7202491760253906\n",
      "\n",
      " EPOCH 13/100 \t train loss 4.319706439971924 \t \t val loss 3.936246871948242\n",
      "\n",
      " EPOCH 14/100 \t train loss 4.027631759643555 \t \t val loss 2.928447723388672\n",
      "\n",
      " EPOCH 15/100 \t train loss 3.885474443435669 \t \t val loss 3.004199743270874\n",
      "\n",
      " EPOCH 16/100 \t train loss 3.311770439147949 \t \t val loss 2.7572786808013916\n",
      "\n",
      " EPOCH 17/100 \t train loss 3.003913164138794 \t \t val loss 3.076162576675415\n",
      "\n",
      " EPOCH 18/100 \t train loss 2.7489585876464844 \t \t val loss 2.270490884780884\n",
      "\n",
      " EPOCH 19/100 \t train loss 2.7789969444274902 \t \t val loss 2.7033894062042236\n",
      "\n",
      " EPOCH 20/100 \t train loss 2.5698728561401367 \t \t val loss 2.4651684761047363\n",
      "\n",
      " EPOCH 21/100 \t train loss 2.366624355316162 \t \t val loss 2.0708272457122803\n",
      "\n",
      " EPOCH 22/100 \t train loss 2.270538806915283 \t \t val loss 2.212554931640625\n",
      "\n",
      " EPOCH 23/100 \t train loss 2.0873324871063232 \t \t val loss 1.8906975984573364\n",
      "\n",
      " EPOCH 24/100 \t train loss 2.068148612976074 \t \t val loss 1.9225406646728516\n",
      "\n",
      " EPOCH 25/100 \t train loss 1.9543440341949463 \t \t val loss 1.8468090295791626\n",
      "\n",
      " EPOCH 26/100 \t train loss 1.7905837297439575 \t \t val loss 2.212094783782959\n",
      "\n",
      " EPOCH 27/100 \t train loss 1.7686767578125 \t \t val loss 1.6033647060394287\n",
      "\n",
      " EPOCH 28/100 \t train loss 1.6714996099472046 \t \t val loss 1.4517573118209839\n",
      "\n",
      " EPOCH 29/100 \t train loss 1.6610798835754395 \t \t val loss 1.3976749181747437\n",
      "\n",
      " EPOCH 30/100 \t train loss 1.5478370189666748 \t \t val loss 1.633360743522644\n",
      "\n",
      " EPOCH 31/100 \t train loss 1.460384488105774 \t \t val loss 1.2732951641082764\n",
      "\n",
      " EPOCH 32/100 \t train loss 1.4373564720153809 \t \t val loss 1.2447965145111084\n",
      "\n",
      " EPOCH 33/100 \t train loss 1.3898223638534546 \t \t val loss 1.2627484798431396\n",
      "\n",
      " EPOCH 34/100 \t train loss 1.3488103151321411 \t \t val loss 1.3453452587127686\n",
      "\n",
      " EPOCH 35/100 \t train loss 1.278931736946106 \t \t val loss 1.2749117612838745\n",
      "\n",
      " EPOCH 36/100 \t train loss 1.2409425973892212 \t \t val loss 1.0265467166900635\n",
      "\n",
      " EPOCH 37/100 \t train loss 1.1943540573120117 \t \t val loss 1.0162467956542969\n",
      "\n",
      " EPOCH 38/100 \t train loss 1.1510740518569946 \t \t val loss 1.0556546449661255\n",
      "\n",
      " EPOCH 39/100 \t train loss 1.172513723373413 \t \t val loss 1.1508630514144897\n",
      "\n",
      " EPOCH 40/100 \t train loss 1.1540446281433105 \t \t val loss 0.9865416288375854\n",
      "\n",
      " EPOCH 41/100 \t train loss 1.0741297006607056 \t \t val loss 1.0767598152160645\n",
      "\n",
      " EPOCH 42/100 \t train loss 1.0623313188552856 \t \t val loss 0.9887645244598389\n",
      "\n",
      " EPOCH 43/100 \t train loss 1.0128322839736938 \t \t val loss 0.9924372434616089\n",
      "\n",
      " EPOCH 44/100 \t train loss 1.039290189743042 \t \t val loss 0.9712214469909668\n",
      "\n",
      " EPOCH 45/100 \t train loss 0.988072395324707 \t \t val loss 1.0422143936157227\n",
      "\n",
      " EPOCH 46/100 \t train loss 1.0133610963821411 \t \t val loss 0.8637869358062744\n",
      "\n",
      " EPOCH 47/100 \t train loss 0.9123262763023376 \t \t val loss 1.0530740022659302\n",
      "\n",
      " EPOCH 48/100 \t train loss 0.915582537651062 \t \t val loss 1.0091601610183716\n",
      "\n",
      " EPOCH 49/100 \t train loss 0.9319562911987305 \t \t val loss 0.8326674103736877\n",
      "\n",
      " EPOCH 50/100 \t train loss 0.8644261956214905 \t \t val loss 0.7608689069747925\n",
      "\n",
      " EPOCH 51/100 \t train loss 0.8491887450218201 \t \t val loss 0.9119042754173279\n",
      "\n",
      " EPOCH 52/100 \t train loss 0.8741695880889893 \t \t val loss 0.8952792286872864\n",
      "\n",
      " EPOCH 53/100 \t train loss 0.8794324994087219 \t \t val loss 0.7622954845428467\n",
      "\n",
      " EPOCH 54/100 \t train loss 0.8393784165382385 \t \t val loss 0.8401623964309692\n",
      "\n",
      " EPOCH 55/100 \t train loss 0.782454788684845 \t \t val loss 0.8109189867973328\n",
      "\n",
      " EPOCH 56/100 \t train loss 0.8504256010055542 \t \t val loss 0.7763763070106506\n",
      "\n",
      " EPOCH 57/100 \t train loss 0.7855255007743835 \t \t val loss 0.7186968326568604\n",
      "\n",
      " EPOCH 58/100 \t train loss 0.7797017693519592 \t \t val loss 0.7375456094741821\n",
      "\n",
      " EPOCH 59/100 \t train loss 0.7281523942947388 \t \t val loss 0.7058029174804688\n",
      "\n",
      " EPOCH 60/100 \t train loss 0.7983373999595642 \t \t val loss 0.8095656037330627\n",
      "\n",
      " EPOCH 61/100 \t train loss 0.8103368282318115 \t \t val loss 0.763542115688324\n",
      "\n",
      " EPOCH 62/100 \t train loss 0.7724142074584961 \t \t val loss 0.7206786870956421\n",
      "\n",
      " EPOCH 63/100 \t train loss 0.6947928071022034 \t \t val loss 0.6540606021881104\n",
      "\n",
      " EPOCH 64/100 \t train loss 0.66677325963974 \t \t val loss 0.6984589099884033\n",
      "\n",
      " EPOCH 65/100 \t train loss 0.6915307641029358 \t \t val loss 0.6743864417076111\n",
      "\n",
      " EPOCH 66/100 \t train loss 0.7259510159492493 \t \t val loss 0.6732990145683289\n",
      "\n",
      " EPOCH 67/100 \t train loss 0.6444876790046692 \t \t val loss 0.7023554444313049\n",
      "\n",
      " EPOCH 68/100 \t train loss 0.6836366653442383 \t \t val loss 0.750000536441803\n",
      "\n",
      " EPOCH 69/100 \t train loss 0.7140593528747559 \t \t val loss 0.8359582424163818\n",
      "\n",
      " EPOCH 70/100 \t train loss 0.6512208580970764 \t \t val loss 0.6341625452041626\n",
      "\n",
      " EPOCH 71/100 \t train loss 0.6336283087730408 \t \t val loss 0.6746730804443359\n",
      "\n",
      " EPOCH 72/100 \t train loss 0.65428227186203 \t \t val loss 0.6321389675140381\n",
      "\n",
      " EPOCH 73/100 \t train loss 0.599975049495697 \t \t val loss 0.6057908535003662\n",
      "\n",
      " EPOCH 74/100 \t train loss 0.662083625793457 \t \t val loss 0.6151562333106995\n",
      "\n",
      " EPOCH 75/100 \t train loss 0.6195526719093323 \t \t val loss 0.6140886545181274\n",
      "\n",
      " EPOCH 76/100 \t train loss 0.6397823691368103 \t \t val loss 0.6775608062744141\n",
      "\n",
      " EPOCH 77/100 \t train loss 0.6545682549476624 \t \t val loss 0.7148004174232483\n",
      "\n",
      " EPOCH 78/100 \t train loss 0.6078024506568909 \t \t val loss 0.6011164784431458\n",
      "\n",
      " EPOCH 79/100 \t train loss 0.5568318963050842 \t \t val loss 0.5715752243995667\n",
      "\n",
      " EPOCH 80/100 \t train loss 0.5620445609092712 \t \t val loss 0.5672457218170166\n",
      "\n",
      " EPOCH 81/100 \t train loss 0.5573149919509888 \t \t val loss 0.699327290058136\n",
      "\n",
      " EPOCH 82/100 \t train loss 0.5905768275260925 \t \t val loss 0.7379375696182251\n",
      "\n",
      " EPOCH 83/100 \t train loss 0.6268439888954163 \t \t val loss 0.7155865430831909\n",
      "\n",
      " EPOCH 84/100 \t train loss 0.5894191265106201 \t \t val loss 0.763788104057312\n",
      "\n",
      " EPOCH 85/100 \t train loss 0.5366520285606384 \t \t val loss 0.636261522769928\n",
      "\n",
      " EPOCH 86/100 \t train loss 0.5412639379501343 \t \t val loss 0.6531931161880493\n",
      "\n",
      " EPOCH 87/100 \t train loss 0.5510477423667908 \t \t val loss 0.5438081622123718\n",
      "\n",
      " EPOCH 88/100 \t train loss 0.5558127760887146 \t \t val loss 0.5454375147819519\n",
      "\n",
      " EPOCH 89/100 \t train loss 0.5451246500015259 \t \t val loss 0.631796658039093\n",
      "\n",
      " EPOCH 90/100 \t train loss 0.5150643587112427 \t \t val loss 0.5552540421485901\n",
      "\n",
      " EPOCH 91/100 \t train loss 0.5317484140396118 \t \t val loss 0.516088604927063\n",
      "\n",
      " EPOCH 92/100 \t train loss 0.498372882604599 \t \t val loss 0.658519983291626\n",
      "\n",
      " EPOCH 93/100 \t train loss 0.5138183832168579 \t \t val loss 0.6145690679550171\n",
      "\n",
      " EPOCH 94/100 \t train loss 0.49059921503067017 \t \t val loss 0.5652115345001221\n",
      "\n",
      " EPOCH 95/100 \t train loss 0.4833980202674866 \t \t val loss 0.5141124725341797\n",
      "\n",
      " EPOCH 96/100 \t train loss 0.55080246925354 \t \t val loss 0.7314820885658264\n",
      "\n",
      " EPOCH 97/100 \t train loss 0.5110713243484497 \t \t val loss 0.5944439768791199\n",
      "\n",
      " EPOCH 98/100 \t train loss 0.4945675730705261 \t \t val loss 0.6285859942436218\n",
      "\n",
      " EPOCH 99/100 \t train loss 0.48707830905914307 \t \t val loss 0.5232976078987122\n",
      "\n",
      " EPOCH 100/100 \t train loss 0.4517572224140167 \t \t val loss 0.5279223918914795\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1000\n",
    "num_epochs = 100\n",
    "diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "decay = 0.95\n",
    "for epoch in range(num_epochs):\n",
    "#     if epoch == 0:\n",
    "#         for param in optim.param_groups:\n",
    "#             param[\"lr\"] = param[\"lr\"]/(10e3)\n",
    "#     elif epoch <= 3:\n",
    "#         for param in optim.param_groups:\n",
    "#             param[\"lr\"] = param[\"lr\"]*10\n",
    "    train_loss = train(gcn, device, train_dataloader, loss_fn, optim)\n",
    "    test_loss = test(gcn, device, test_dataloader, loss_fn)\n",
    "    print('\\n EPOCH {}/{} \\t train loss {} \\t \\t val loss {}'.format(epoch + 1, num_epochs, train_loss, test_loss))\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(test_loss)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(gcn.state_dict(), \"./saved_models/with_resnet_{}.pth\".format(RUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2bc28dd-8c2a-40b1-9bbf-493651a9a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, data ,loss_fn, optimizer, batch_acc=1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(data):\n",
    "        x, edge_index, edge_attr, target, edge_cross, types = (batch[\"x\"],\n",
    "                                                               batch[\"edge_index\"],\n",
    "                                                               batch[\"edge_attr\"],\n",
    "                                                               batch[\"y\"],\n",
    "                                                               batch[\"nodes_target\"],\n",
    "                                                               batch[\"types\"])\n",
    "        types_cpu = types.numpy()\n",
    "        sort_index = torch.Tensor(types.numpy().argsort(kind=\"stable\")).long()\n",
    "        target = target[sort_index]\n",
    "        x, edge_index, edge_attr, target, edge_cross, types = x.to(device), \\\n",
    "                                                            edge_index.long().to(device), \\\n",
    "                                                            edge_attr.to(device), \\\n",
    "                                                            target.to(device),\\\n",
    "                                                            edge_cross.long().to(device), \\\n",
    "                                                            types.long().to(device)\n",
    "        logits = model(x, edge_index, edge_attr, edge_cross, types)\n",
    "        loss = loss_fn()\n",
    "        output=loss(logits.squeeze(), target.squeeze())\n",
    "        output.backward()\n",
    "        if batch_acc == 1:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "        elif (i%batch_acc == 0) and (i != 0):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "        train_loss = output.data.cpu().numpy()\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0bc75cc-5071-44ab-93aa-41e3ae040e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EPOCH 1/100 \t train loss 0.530784547328949 \t \t val loss 0.6098902225494385\n",
      "\n",
      " EPOCH 2/100 \t train loss 0.4895051121711731 \t \t val loss 0.5317975282669067\n",
      "\n",
      " EPOCH 3/100 \t train loss 0.44453608989715576 \t \t val loss 0.5469436645507812\n",
      "\n",
      " EPOCH 4/100 \t train loss 0.47759273648262024 \t \t val loss 0.6072366237640381\n",
      "\n",
      " EPOCH 5/100 \t train loss 0.4598345160484314 \t \t val loss 0.5116789937019348\n",
      "\n",
      " EPOCH 6/100 \t train loss 0.5224311351776123 \t \t val loss 0.6302801966667175\n",
      "\n",
      " EPOCH 7/100 \t train loss 0.45595717430114746 \t \t val loss 0.545138955116272\n",
      "\n",
      " EPOCH 8/100 \t train loss 0.4223570227622986 \t \t val loss 0.4845689833164215\n",
      "\n",
      " EPOCH 9/100 \t train loss 0.45652061700820923 \t \t val loss 0.6576845645904541\n",
      "\n",
      " EPOCH 10/100 \t train loss 0.4898805022239685 \t \t val loss 0.5497469305992126\n",
      "\n",
      " EPOCH 11/100 \t train loss 0.4196653962135315 \t \t val loss 0.4888512194156647\n",
      "\n",
      " EPOCH 12/100 \t train loss 0.41594198346138 \t \t val loss 0.4935818612575531\n",
      "\n",
      " EPOCH 13/100 \t train loss 0.5658427476882935 \t \t val loss 0.5029827952384949\n",
      "\n",
      " EPOCH 14/100 \t train loss 0.4085651934146881 \t \t val loss 0.49600833654403687\n",
      "\n",
      " EPOCH 15/100 \t train loss 0.40017005801200867 \t \t val loss 0.5271227359771729\n",
      "\n",
      " EPOCH 16/100 \t train loss 0.4178121089935303 \t \t val loss 0.5008909106254578\n",
      "\n",
      " EPOCH 17/100 \t train loss 0.4272303879261017 \t \t val loss 0.5704889893531799\n",
      "\n",
      " EPOCH 18/100 \t train loss 0.4551970958709717 \t \t val loss 0.5982468724250793\n",
      "\n",
      " EPOCH 19/100 \t train loss 0.4770086407661438 \t \t val loss 0.6796188950538635\n",
      "\n",
      " EPOCH 20/100 \t train loss 0.4657268226146698 \t \t val loss 0.4811503291130066\n",
      "\n",
      " EPOCH 21/100 \t train loss 0.3576613962650299 \t \t val loss 0.43470996618270874\n",
      "\n",
      " EPOCH 22/100 \t train loss 0.3427863121032715 \t \t val loss 0.48771074414253235\n",
      "\n",
      " EPOCH 23/100 \t train loss 0.33884742856025696 \t \t val loss 0.43759575486183167\n",
      "\n",
      " EPOCH 24/100 \t train loss 0.3355100750923157 \t \t val loss 0.4357835054397583\n",
      "\n",
      " EPOCH 25/100 \t train loss 0.3328540623188019 \t \t val loss 0.4647068679332733\n",
      "\n",
      " EPOCH 26/100 \t train loss 0.3331942856311798 \t \t val loss 0.43735820055007935\n",
      "\n",
      " EPOCH 27/100 \t train loss 0.3376327157020569 \t \t val loss 0.44715720415115356\n",
      "\n",
      " EPOCH 28/100 \t train loss 0.3353642225265503 \t \t val loss 0.4883390963077545\n",
      "\n",
      " EPOCH 29/100 \t train loss 0.3367120623588562 \t \t val loss 0.46283334493637085\n",
      "\n",
      " EPOCH 30/100 \t train loss 0.33473408222198486 \t \t val loss 0.44028669595718384\n",
      "\n",
      " EPOCH 31/100 \t train loss 0.39557865262031555 \t \t val loss 0.5514200925827026\n",
      "\n",
      " EPOCH 32/100 \t train loss 0.39250919222831726 \t \t val loss 0.47684621810913086\n",
      "\n",
      " EPOCH 33/100 \t train loss 0.3305565118789673 \t \t val loss 0.44427773356437683\n",
      "\n",
      " EPOCH 34/100 \t train loss 0.32637831568717957 \t \t val loss 0.42086145281791687\n",
      "\n",
      " EPOCH 35/100 \t train loss 0.3199104964733124 \t \t val loss 0.4908887445926666\n",
      "\n",
      " EPOCH 36/100 \t train loss 0.318692147731781 \t \t val loss 0.4419282376766205\n",
      "\n",
      " EPOCH 37/100 \t train loss 0.31729647517204285 \t \t val loss 0.4149459898471832\n",
      "\n",
      " EPOCH 38/100 \t train loss 0.31781139969825745 \t \t val loss 0.47559913992881775\n",
      "\n",
      " EPOCH 39/100 \t train loss 0.32174816727638245 \t \t val loss 0.4381910562515259\n",
      "\n",
      " EPOCH 40/100 \t train loss 0.3217924237251282 \t \t val loss 0.4302828311920166\n",
      "\n",
      " EPOCH 41/100 \t train loss 0.3033069372177124 \t \t val loss 0.45151785016059875\n",
      "\n",
      " EPOCH 42/100 \t train loss 0.2987961173057556 \t \t val loss 0.4290883541107178\n",
      "\n",
      " EPOCH 43/100 \t train loss 0.30454301834106445 \t \t val loss 0.45883527398109436\n",
      "\n",
      " EPOCH 44/100 \t train loss 0.3434973955154419 \t \t val loss 0.4165305495262146\n",
      "\n",
      " EPOCH 45/100 \t train loss 0.28233131766319275 \t \t val loss 0.40612277388572693\n",
      "\n",
      " EPOCH 46/100 \t train loss 0.2689957022666931 \t \t val loss 0.3928362727165222\n",
      "\n",
      " EPOCH 47/100 \t train loss 0.27478983998298645 \t \t val loss 0.40767958760261536\n",
      "\n",
      " EPOCH 48/100 \t train loss 0.26930034160614014 \t \t val loss 0.39105483889579773\n",
      "\n",
      " EPOCH 49/100 \t train loss 0.2651185989379883 \t \t val loss 0.39467906951904297\n",
      "\n",
      " EPOCH 50/100 \t train loss 0.2637435793876648 \t \t val loss 0.4022360146045685\n",
      "\n",
      " EPOCH 51/100 \t train loss 0.2678326666355133 \t \t val loss 0.3992857038974762\n",
      "\n",
      " EPOCH 52/100 \t train loss 0.26205629110336304 \t \t val loss 0.3838392496109009\n",
      "\n",
      " EPOCH 53/100 \t train loss 0.2577509582042694 \t \t val loss 0.39576905965805054\n",
      "\n",
      " EPOCH 54/100 \t train loss 0.25754496455192566 \t \t val loss 0.3830872178077698\n",
      "\n",
      " EPOCH 55/100 \t train loss 0.25608810782432556 \t \t val loss 0.3914061188697815\n",
      "\n",
      " EPOCH 56/100 \t train loss 0.25959211587905884 \t \t val loss 0.3831327259540558\n",
      "\n",
      " EPOCH 57/100 \t train loss 0.2567537724971771 \t \t val loss 0.39055684208869934\n",
      "\n",
      " EPOCH 58/100 \t train loss 0.2589591145515442 \t \t val loss 0.38693881034851074\n",
      "\n",
      " EPOCH 59/100 \t train loss 0.25479575991630554 \t \t val loss 0.38165920972824097\n",
      "\n",
      " EPOCH 60/100 \t train loss 0.25339218974113464 \t \t val loss 0.3908977508544922\n",
      "\n",
      " EPOCH 61/100 \t train loss 0.24327363073825836 \t \t val loss 0.38403546810150146\n",
      "\n",
      " EPOCH 62/100 \t train loss 0.23796872794628143 \t \t val loss 0.3812926709651947\n",
      "\n",
      " EPOCH 63/100 \t train loss 0.23537340760231018 \t \t val loss 0.37477150559425354\n",
      "\n",
      " EPOCH 64/100 \t train loss 0.23479601740837097 \t \t val loss 0.36799338459968567\n",
      "\n",
      " EPOCH 65/100 \t train loss 0.23423261940479279 \t \t val loss 0.38761141896247864\n",
      "\n",
      " EPOCH 66/100 \t train loss 0.23379601538181305 \t \t val loss 0.3779565393924713\n",
      "\n",
      " EPOCH 67/100 \t train loss 0.2341795414686203 \t \t val loss 0.3723579943180084\n",
      "\n",
      " EPOCH 68/100 \t train loss 0.23409432172775269 \t \t val loss 0.3721109628677368\n",
      "\n",
      " EPOCH 69/100 \t train loss 0.23316311836242676 \t \t val loss 0.3853205442428589\n",
      "\n",
      " EPOCH 70/100 \t train loss 0.23311887681484222 \t \t val loss 0.3772328495979309\n",
      "\n",
      " EPOCH 71/100 \t train loss 0.23171746730804443 \t \t val loss 0.37882283329963684\n",
      "\n",
      " EPOCH 72/100 \t train loss 0.2314257174730301 \t \t val loss 0.37381237745285034\n",
      "\n",
      " EPOCH 73/100 \t train loss 0.2308105230331421 \t \t val loss 0.3818679749965668\n",
      "\n",
      " EPOCH 74/100 \t train loss 0.2298402637243271 \t \t val loss 0.36861491203308105\n",
      "\n",
      " EPOCH 75/100 \t train loss 0.2299497425556183 \t \t val loss 0.37220636010169983\n",
      "\n",
      " EPOCH 76/100 \t train loss 0.2296629399061203 \t \t val loss 0.3688272535800934\n",
      "\n",
      " EPOCH 77/100 \t train loss 0.2288178652524948 \t \t val loss 0.3825376331806183\n",
      "\n",
      " EPOCH 78/100 \t train loss 0.22769905626773834 \t \t val loss 0.3748801052570343\n",
      "\n",
      " EPOCH 79/100 \t train loss 0.22670891880989075 \t \t val loss 0.3677518665790558\n",
      "\n",
      " EPOCH 80/100 \t train loss 0.22969521582126617 \t \t val loss 0.3722723126411438\n",
      "\n",
      " EPOCH 81/100 \t train loss 0.2258118838071823 \t \t val loss 0.3716358542442322\n",
      "\n",
      " EPOCH 82/100 \t train loss 0.22127816081047058 \t \t val loss 0.36256060004234314\n",
      "\n",
      " EPOCH 83/100 \t train loss 0.22056043148040771 \t \t val loss 0.37481340765953064\n",
      "\n",
      " EPOCH 84/100 \t train loss 0.21985702216625214 \t \t val loss 0.36267825961112976\n",
      "\n",
      " EPOCH 85/100 \t train loss 0.21904073655605316 \t \t val loss 0.37429332733154297\n",
      "\n",
      " EPOCH 86/100 \t train loss 0.21816396713256836 \t \t val loss 0.36292245984077454\n",
      "\n",
      " EPOCH 87/100 \t train loss 0.2177562415599823 \t \t val loss 0.36654266715049744\n",
      "\n",
      " EPOCH 88/100 \t train loss 0.21747678518295288 \t \t val loss 0.36410075426101685\n",
      "\n",
      " EPOCH 89/100 \t train loss 0.21752893924713135 \t \t val loss 0.3681029975414276\n",
      "\n",
      " EPOCH 90/100 \t train loss 0.21784941852092743 \t \t val loss 0.3669850528240204\n",
      "\n",
      " EPOCH 91/100 \t train loss 0.2169431895017624 \t \t val loss 0.36633408069610596\n",
      "\n",
      " EPOCH 92/100 \t train loss 0.2170255035161972 \t \t val loss 0.3671993613243103\n",
      "\n",
      " EPOCH 93/100 \t train loss 0.21743519604206085 \t \t val loss 0.367860347032547\n",
      "\n",
      " EPOCH 94/100 \t train loss 0.21826043725013733 \t \t val loss 0.369461327791214\n",
      "\n",
      " EPOCH 95/100 \t train loss 0.21880167722702026 \t \t val loss 0.37035802006721497\n",
      "\n",
      " EPOCH 96/100 \t train loss 0.21861374378204346 \t \t val loss 0.36592450737953186\n",
      "\n",
      " EPOCH 97/100 \t train loss 0.2177191823720932 \t \t val loss 0.3726976811885834\n",
      "\n",
      " EPOCH 98/100 \t train loss 0.21730658411979675 \t \t val loss 0.36126700043678284\n",
      "\n",
      " EPOCH 99/100 \t train loss 0.2165105789899826 \t \t val loss 0.3793494701385498\n",
      "\n",
      " EPOCH 100/100 \t train loss 0.2150755375623703 \t \t val loss 0.35929402709007263\n"
     ]
    }
   ],
   "source": [
    "batch_acc = 1\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % 20 == 0:\n",
    "        batch_acc *= 2\n",
    "        for param in optim.param_groups:\n",
    "            param[\"weight_decay\"] = param[\"weight_decay\"]/2\n",
    "    train_loss = train(gcn, device, train_dataloader, loss_fn, optim, batch_acc)\n",
    "    test_loss = test(gcn, device, test_dataloader, loss_fn)\n",
    "    print('\\n EPOCH {}/{} \\t train loss {} \\t \\t val loss {}'.format(epoch + 1, num_epochs, train_loss, test_loss))\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(test_loss)\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(gcn.state_dict(), \"./saved_models/with_resnet_{}.pth\".format(RUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cdc9de2-1a6b-4df9-820e-edbee6326cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-26-23:59:40_lr=0.00078_wd=1.76755e-08_p=0.0343089_conv_features=128_n_layers=3_n_res=3\n"
     ]
    }
   ],
   "source": [
    "print(RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b5d3da6-266d-4864-a3bd-e3a24f2c59ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./train_dataloader.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(train_dataloader, f)\n",
    "# with open(\"./test_dataloader.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(test_dataloader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39fa38-b137-47a7-ae91-990bda57b4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi_python",
   "language": "python",
   "name": "mpi_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
