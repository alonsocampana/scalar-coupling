{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fd615a-450f-46f1-ab39-b6fcef802df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url, Data, Batch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mendeleev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e36dfec-0763-4e55-be3a-e90b5ecd89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mendeleev import element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7e0a8f-6d98-4bf3-9295-b423e279dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, H, N, O, F = element([\"C\", \"H\", \"N\", \"O\", \"F\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b06396-df25-4d9b-bd0b-476c3107033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for e in [C, H, N, O, F]:\n",
    "    row = [e.atomic_radius,\n",
    "           e.atomic_volume,\n",
    "           e.atomic_weight,\n",
    "           e.boiling_point,\n",
    "           e.covalent_radius_bragg,\n",
    "           e.dipole_polarizability,\n",
    "           e.electron_affinity,\n",
    "           e.en_ghosh,\n",
    "           e.vdw_radius]\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "893b4d2d-655b-435e-b955-c06190188fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties=np.array(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d88aec5-a3e5-40f4-85f9-e4246345bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties[properties == None] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9022d475-1227-4fcf-9f82-f49b6174f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "emms = MinMaxScaler([-5, 5])\n",
    "props = properties.astype(np.float32)\n",
    "props = emms.fit_transform(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10764aa2-7d94-479d-b435-2b2625c90271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metaedge_features(incident_edges, coords):\n",
    "    head2tail = []\n",
    "    triads_ht = []\n",
    "    head2head = []\n",
    "    triads_hh = []\n",
    "    tail2tail = []\n",
    "    triads_tt = []\n",
    "    for x, i in enumerate(incident_edges):\n",
    "        for y, j in enumerate(incident_edges):\n",
    "            if i[1] == j[0]:\n",
    "                head2tail.append((x, y))\n",
    "                triads_ht.append((i[0], i[1], j[1]))\n",
    "            if i[0] == j[0] and x != y:\n",
    "                tail2tail.append((x, y))\n",
    "                triads_hh.append((i[1], i[0], j[1]))\n",
    "            if i[1] == j[1] and x != y:\n",
    "                head2head.append((x, y))\n",
    "                triads_tt.append((i[0], i[1], j[0]))\n",
    "\n",
    "    head2tail = np.array(head2tail)\n",
    "    head2head = np.array(head2head)\n",
    "    tail2tail = np.array(tail2tail)\n",
    "    all_triads = triads_ht + triads_hh + triads_tt\n",
    "    non_empty = [array for array in [head2tail, head2head, tail2tail] if len(array) > 0]\n",
    "    metaedges = np.concatenate(non_empty)\n",
    "    angles = []\n",
    "    for triad in all_triads:\n",
    "        angles.append(get_angle(coords, triad))\n",
    "    return metaedges, np.array(angles)\n",
    "\n",
    "def get_angle(array, triad):\n",
    "    \"\"\"\n",
    "    Array of distances to angles between edges\n",
    "    \"\"\"\n",
    "    i, j, k = triad\n",
    "    a =  array[i] - array[j]\n",
    "    b =  array[k] - array[j]\n",
    "    a = a/np.linalg.norm(a)\n",
    "    b = b/np.linalg.norm(b)\n",
    "    return a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11280e70-1350-4fdc-b5ea-594205942b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_features(coords, dipole_moment, eps = 0.000001):\n",
    "    norm_dipole = np.linalg.norm(dipole_moment) # get the norm to normalize the vector and find angles\n",
    "    distmat = squareform(pdist(coords)) # get dist_mat to find edges\n",
    "    np.fill_diagonal(distmat, np.nan) # fill to avoid ranking problem\n",
    "    rankings = distmat.argsort(axis=0) # order distance matrix to get n-neighborhood of each edge\n",
    "    G = nx.from_numpy_matrix(distmat, create_using=nx.DiGraph())\n",
    "    G.remove_edges_from(nx.selfloop_edges(G)) # remove to avoid self edges\n",
    "    edgelist = nx.to_edgelist(G)\n",
    "    edgelist, edge_features = edge_list_to_numpy(edgelist) #get edges and distances\n",
    "    G = nx.from_numpy_matrix(rankings+1, create_using=nx.DiGraph())\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    edgelist2 = nx.to_edgelist(G)\n",
    "    edgelist2, edge_rankings = edge_list_to_numpy(edgelist2) # get rankings\n",
    "    edge_rankings = edge_rankings - 1\n",
    "    n_edges = edgelist.shape[0]\n",
    "    coords_nodes = coords[edgelist] #select nodes in edges\n",
    "    vectors_edges = coords_nodes.transpose(0, 2, 1)[:, :, 1] - coords_nodes.transpose(0, 2, 1)[:, :, 0] # get vector of each edge\n",
    "    vectors_edges_normalized = vectors_edges/(np.linalg.norm(vectors_edges, axis=1) + eps)[:,None] \n",
    "    dipole_moment_normalized = dipole_moment/(norm_dipole + eps)\n",
    "    angles_dipole_moment = np.dot(vectors_edges_normalized, dipole_moment) # do dot product to find angles\n",
    "    ranks = np.zeros([n_edges, 6])\n",
    "    edge_rankings[edge_rankings > 4] = 5 # replace to impose same dimensionality in all molecules\n",
    "    ranks[np.arange(n_edges), edge_rankings] = 1\n",
    "    edge_features = np.concatenate([edge_features[:,None], ranks, angles_dipole_moment[:,None]], axis=1) # concatenate all features\n",
    "    return edgelist, edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10999a66-5f37-4f67-bf94-4b9b1ea21329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_list_to_numpy(edgelist):\n",
    "    tail = []\n",
    "    head = []\n",
    "    weight = []\n",
    "    for edge in list(edgelist):\n",
    "        tail.append(edge[0])\n",
    "        head.append(edge[1])\n",
    "        weight.append(edge[2][\"weight\"])\n",
    "    tail = np.array(tail)[:,None]\n",
    "    head = np.array(head)[:,None]\n",
    "    weight = np.array(weight)\n",
    "    return np.concatenate([tail, head], axis=1), weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce54f832-c04b-46b1-b7de-e8e123d761c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"raw/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fefd4fb-b9cc-43f7-b411-2ad0017f6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = target[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae7f5252-fc4d-47fb-92c0-7104debc580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_to_int = {edges[i]:i for i in range(len(edges))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1629b297-4240-4bf6-9d19-c6e7c60a01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, training= True):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.filenames = pd.read_csv(\"raw/processed_names.csv\")\n",
    "        self.charges = pd.read_csv(\"raw/mulliken_charges.csv\")\n",
    "        self.magnetic_shieldings = pd.read_csv(\"raw/magnetic_shielding_tensors.csv\")\n",
    "        self.dipole_moments = pd.read_csv(\"raw/dipole_moments.csv\")\n",
    "        self.potential_energy = pd.read_csv(\"raw/potential_energy.csv\")\n",
    "        self.target = pd.read_csv(\"raw/train.csv\")\n",
    "        self.structures = pd.read_csv(\"raw/structures.csv\")\n",
    "        self.molecule_names = molecule_names = np.unique(self.potential_energy[\"molecule_name\"])\n",
    "        if training:\n",
    "            self.training_mask = np.loadtxt(\"./raw/training_mask2.csv\").astype(bool)\n",
    "            self.molecule_names = self.molecule_names[self.training_mask]\n",
    "            \n",
    "    def len(self) -> int:\n",
    "        return len(self.molecule_names)\n",
    "    def standarize(self):\n",
    "        mms = MinMaxScaler([-4, 4])\n",
    "        self.charges[\"mulliken_charge\"] = mms.fit_transform(self.charges[[\"mulliken_charge\"]]).squeeze()\n",
    "        self.magnetic_shieldings.iloc[:, 2:] = mms.fit_transform(self.magnetic_shieldings.iloc[:, 2:])\n",
    "        self.dipole_moments.iloc[:, 1:] = mms.fit_transform(self.dipole_moments.iloc[:, 1:])\n",
    "        self.potential_energy[\"potential_energy\"] = mms.fit_transform(self.potential_energy[[\"potential_energy\"]]).squeeze()\n",
    "\n",
    "    def preprocess(self, k = None):\n",
    "        charges = self.charges\n",
    "        magnetic_shieldings = self.magnetic_shieldings\n",
    "        dipole_moments = self.dipole_moments\n",
    "        potential_energy = self.potential_energy\n",
    "        molecule_names = self.molecule_names\n",
    "        target = self.target\n",
    "        structures = self.structures\n",
    "        dfs = [charges, magnetic_shieldings, dipole_moments, potential_energy, target, structures]\n",
    "        for i in range(len(dfs)):\n",
    "            dfs[i] = dfs[i].set_index(\"molecule_name\", drop=True)\n",
    "        charges, magnetic_shieldings, dipole_moments, potential_energy, target, structures = dfs\n",
    "        atoms = structures[\"atom\"].unique()\n",
    "        atoms_id = {atoms[i]:i for i in range(len(atoms))}\n",
    "        training_mask = []\n",
    "        for x, name in enumerate(list(molecule_names)):\n",
    "            any_training_edges = True\n",
    "            coords = structures.loc[name][[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "            n_nodes = coords.shape[0]\n",
    "            print(\"{}/{}\".format(x + 1, len(molecule_names)), end = \"\\r\")\n",
    "            # adj_mat\n",
    "            with open(\"./processed/{}_adj_mat.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, get_distance_matrix(coords, k))\n",
    "            atom_types = structures.loc[name][\"atom\"].replace(atoms_id).to_numpy()\n",
    "            atom_onehot = np.zeros([n_nodes, len(atoms)])\n",
    "            atom_onehot[np.arange(0, n_nodes), atom_types] = 1\n",
    "            charge = charges.loc[name][\"mulliken_charge\"].to_numpy()\n",
    "            shieldings = magnetic_shieldings.loc[name].iloc[:, 2:].to_numpy()\n",
    "            node_features = np.concatenate([charge[:, None], shieldings, atom_onehot], axis=1)\n",
    "            with open(\"./processed/{}_node_attr.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, node_features)\n",
    "            try:\n",
    "                edges_target = target.loc[[name]]\n",
    "                training_mask.append(True)\n",
    "            except KeyError:\n",
    "                training_mask.append(False)\n",
    "                any_training_edges = False\n",
    "                \n",
    "            if any_training_edges:\n",
    "                edges_target[\"type\"] = edges_target[\"type\"].replace(edge_to_int).astype(np.int64)\n",
    "                scalar_coupling = edges_target.loc[:, [\"atom_index_0\", \"atom_index_1\",\"type\",\"scalar_coupling_constant\"]].to_numpy()\n",
    "            else:\n",
    "                scalar_coupling = np.array([-1, -1, -1, 0])\n",
    "            with open(\"./processed/{}_target.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, scalar_coupling)\n",
    "            # Graph features\n",
    "            dipole_moment = dipole_moments.loc[name]\n",
    "            norm_dipole = np.array([np.linalg.norm(dipole_moment)])\n",
    "            potential = potential_energy.loc[name]\n",
    "            graph_features = (np.concatenate([dipole_moment, norm_dipole, potential, np.array([n_nodes])]))\n",
    "            with open(\"./processed/{}_graph_feautures.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, graph_features)\n",
    "            # edge_features\n",
    "            edgelist, edgeattr = get_edge_features(coords, dipole_moment)\n",
    "            with open(\"./processed/{}_edge_list.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, edgelist)\n",
    "            with open(\"./processed/{}_edge_attr.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, edgeattr)\n",
    "        with open(\"./raw/training_mask2.csv\".format(name), \"wb\") as f:\n",
    "                np.savetxt(f, np.array(training_mask))\n",
    "                \n",
    "    def mem_load(self):\n",
    "        self.mem = {}\n",
    "        for i, molecule in enumerate(self.molecule_names):\n",
    "            graph_features = pd.read_csv(\"./processed/{}_graph_feautures.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            node_features = pd.read_csv(\"./processed/{}_node_attr.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            atomtypes = node_features[:,-5:].argmax(axis=1)\n",
    "            prop_atoms = props[atomtypes,:]\n",
    "            n_nodes = node_features.shape[0]\n",
    "            graph_features = np.tile(graph_features, [1, n_nodes]).T\n",
    "            node_features = np.concatenate([prop_atoms, node_features, graph_features], axis=1)\n",
    "            target =  pd.read_csv(\"./processed/{}_target.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            edge_type = target[:,2]\n",
    "            edge_type = np.concatenate([edge_type, edge_type], axis=0)\n",
    "            edges_target = target[:,0:2]\n",
    "            target = target[:,3]\n",
    "            target = np.concatenate([target, target])\n",
    "            edges_target = np.concatenate([edges_target, edges_target[:,::-1]], axis=0)\n",
    "            edge_list = pd.read_csv(\"./processed/{}_edge_list.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            edge_attr = pd.read_csv(\"./processed/{}_edge_attr.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            metaedge_list = pd.read_csv(\"./processed/{}_metaedge_list.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            metaedge_attr = pd.read_csv(\"./processed/{}_metaedge_attr.csv\".format(molecule), sep=\" \", header=None).to_numpy()\n",
    "            data = Data(x=torch.Tensor(node_features), edge_index = torch.Tensor(edge_list).T, y=torch.Tensor(target), edge_attr = torch.Tensor(edge_attr))\n",
    "            data.nodes_target = torch.Tensor(edges_target)\n",
    "            data.nodes = n_nodes\n",
    "            data.edges = edge_list.shape[0]\n",
    "            data.types = torch.Tensor(edge_type)\n",
    "            data.metaedge_list = metaedge_list\n",
    "            data.metaedge_list = metaedge_attrS\n",
    "            self.mem[molecule] = data\n",
    "            print(\"{}/{}\".format(i, len(self.molecule_names)), end = \"\\r\")\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        molecule = self.molecule_names[idx]\n",
    "        return self.mem[molecule]\n",
    "        \n",
    "\n",
    "def get_distance_matrix(X, k=None):\n",
    "    dist = squareform(pdist(X))\n",
    "    if k is not None:\n",
    "        non_k = dist.argsort(axis=1)[:, k+1:]\n",
    "        dist[np.arange(0, dist.shape[0])[:,None], non_k] = 0\n",
    "    return dist\n",
    "\n",
    "def to_batch(list_graphs):\n",
    "    n_nodes = 0\n",
    "    for graph in list_graphs:\n",
    "        graph[\"nodes_target\"] += n_nodes\n",
    "        n_nodes += graph.nodes\n",
    "    return Batch.from_data_list(list_graphs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e2a3228-c249-4a99-9a85-e8b34964c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./train_dataloader.pkl\", \"rb\") as f:\n",
    "    train_dataloader = pickle.load(f)\n",
    "with open(\"./test_dataloader.pkl\", \"rb\") as f:\n",
    "    test_dataloader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90e9f941-3bb4-46cd-b74c-fa0efc63b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85c4a9cf-0e2a-48fa-b2a3-1085e0dafbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConvEncoderGated(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_features, heads, n_layers, p_dropout):\n",
    "        super().__init__()\n",
    "        self.p_dropout = p_dropout\n",
    "        assert n_layers > 1\n",
    "        self.init_conv = TransformerConv(num_node_features, hidden_features, heads=n_heads, dropout=p_dropout,  edge_dim=8, concat=False)\n",
    "        self.layers = nn.ModuleList([TransformerConv(hidden_features, hidden_features, heads=n_heads, dropout=p_dropout,  edge_dim=8, concat=False) for i in range(n_layers)])\n",
    "        self.gates = nn.Parameter(torch.Tensor(n_layers))\n",
    "        self.init_conv.apply(init_weights)\n",
    "        for conv in self.layers:\n",
    "            conv.apply(init_weights)\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        range_gates = torch.sigmoid(self.gates)\n",
    "        x = self.init_conv(x, edge_index, edge_attr)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.leaky_relu(x)\n",
    "            x = (range_gates[i])*layer(x, edge_index, edge_attr) + (1-range_gates[i])*x\n",
    "        return x\n",
    "    \n",
    "class ResNetGated(nn.Module):\n",
    "    def __init__(self, init_dim, hidden_dim, layers, p_dropout):\n",
    "        super().__init__()\n",
    "        self.p_dropout = p_dropout\n",
    "        assert n_layers > 1\n",
    "        self.layers = nn.ModuleList([nn.Sequential(nn.Linear(init_dim, hidden_dim),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Dropout(p=p_dropout),\n",
    "                             nn.Linear( hidden_dim, init_dim)) for i in range(layers)])\n",
    "        self.gates = nn.Parameter(torch.Tensor(n_layers))\n",
    "        self.layers.apply(init_weights)\n",
    "    def forward(self, x):\n",
    "        range_gates = torch.sigmoid(self.gates)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(x)\n",
    "            x = (range_gates[i])*layer(x) + (1-range_gates[i])*x\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, out_features, n_heads, n_layers, n_res, p_dropout):\n",
    "        super().__init__()\n",
    "        # self.conv = TransformerConvEncoderGated(num_node_features, out_features, heads=n_heads, p_dropout=p_dropout,  n_layers=3)\n",
    "        self.fcs = nn.ModuleList([nn.Sequential(ResNetGated(num_node_features*2, out_features*64, n_res, p_dropout),\n",
    "                               nn.Linear(num_node_features*2, 1)) for i in range(8)])\n",
    "        for fc in self.fcs:\n",
    "            fc.apply(init_weights)\n",
    "    def forward(self, x, edge_index, edge_attr, edge_cross, types):\n",
    "        # x = self.conv(x, edge_index, edge_attr)\n",
    "        x = x[edge_cross]\n",
    "        shp = x.shape\n",
    "        x = x.transpose(1, 2).reshape([shp[0], shp[2]*2])\n",
    "        xs = []\n",
    "        for i in range(8):\n",
    "            xs.append(self.fcs[i](x[types == i]))\n",
    "        x = torch.concat(xs, axis=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ce230f5-ad68-46ae-9dda-d7717bf12ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (fcs): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=58, out_features=1, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=58, out_features=1, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=58, out_features=1, bias=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=58, out_features=1, bias=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=58, out_features=1, bias=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=58, out_features=1, bias=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=58, out_features=1, bias=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): ResNetGated(\n",
       "        (layers): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=58, out_features=8192, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Dropout(p=0.001, inplace=False)\n",
       "            (3): Linear(in_features=8192, out_features=58, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(in_features=58, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define the loss function\n",
    "loss_fn = nn.MSELoss\n",
    "\n",
    "lr= 0.001\n",
    "weight_decay = 0.000001\n",
    "p_dropout = 0.001\n",
    "conv_features = 128\n",
    "n_heads = 6\n",
    "n_layers = 3\n",
    "n_res = 2\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "gcn = GCN(29, conv_features, n_heads, n_layers, n_res, p_dropout=p_dropout)\n",
    "params_to_optimize = [\n",
    "    {'params': gcn.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=weight_decay)\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "gcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d37264b3-fab5-459d-ac0c-80187b32c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "RUN = date + \"_lr={}_wd={}_p={}_conv_features={}_n_layers={}_n_res={}\".format(lr,\n",
    "                                                            weight_decay,\n",
    "                                                            p_dropout,\n",
    "                                                            conv_features,\n",
    "                                                             n_layers,\n",
    "                                                            n_res\n",
    "                                                            )\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"./runs/{}\".format(RUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63ada3f4-36ba-41d6-bdca-6966e538843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, data ,loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(data):\n",
    "        x, edge_index, edge_attr, target, edge_cross, types = (batch[\"x\"],\n",
    "                                                               batch[\"edge_index\"],\n",
    "                                                               batch[\"edge_attr\"],\n",
    "                                                               batch[\"y\"],\n",
    "                                                               batch[\"nodes_target\"],\n",
    "                                                               batch[\"types\"])\n",
    "        types_cpu = types.numpy()\n",
    "        sort_index = torch.Tensor(types.numpy().argsort(kind=\"stable\")).long()\n",
    "        target = target[sort_index]\n",
    "        x, edge_index, edge_attr, target, edge_cross, types = x.to(device), \\\n",
    "                                                            edge_index.long().to(device), \\\n",
    "                                                            edge_attr.to(device), \\\n",
    "                                                            target.to(device),\\\n",
    "                                                            edge_cross.long().to(device), \\\n",
    "                                                            types.long().to(device)\n",
    "        logits = model(x, edge_index, edge_attr, edge_cross, types)\n",
    "        loss = loss_fn()\n",
    "        output=loss(logits.squeeze(), target.squeeze())\n",
    "        output.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "        optimizer.step()\n",
    "        train_loss = output.data.cpu().numpy()\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "    return np.mean(train_losses)\n",
    "\n",
    "### Testing function\n",
    "def test(model, device, data, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for i, batch in enumerate(data):\n",
    "            x, edge_index, edge_attr, target, edge_cross, types = (batch[\"x\"],\n",
    "                                                               batch[\"edge_index\"],\n",
    "                                                               batch[\"edge_attr\"],\n",
    "                                                               batch[\"y\"],\n",
    "                                                               batch[\"nodes_target\"],\n",
    "                                                               batch[\"types\"])\n",
    "            types_cpu = types.numpy()\n",
    "            sort_index = torch.Tensor(types.numpy().argsort(kind=\"stable\")).long()\n",
    "            target = target[sort_index]\n",
    "            x, edge_index, edge_attr, target, edge_cross, types = x.to(device), \\\n",
    "                                                                edge_index.long().to(device), \\\n",
    "                                                                edge_attr.to(device), \\\n",
    "                                                                target.to(device),\\\n",
    "                                                                edge_cross.long().to(device), \\\n",
    "                                                                types.long().to(device)\n",
    "            logits = model(x, edge_index, edge_attr, edge_cross, types)\n",
    "            loss = loss_fn()\n",
    "            output=loss(logits.squeeze(), target.squeeze())\n",
    "            test_loss = output.data.cpu().numpy()\n",
    "            test_losses.append(test_loss)\n",
    "    return np.mean(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dcf76ea-d1f1-4b37-a45f-85cfeaf74b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EPOCH 1/100 \t train loss 87.97037506103516 \t \t val loss 45.93452453613281\n",
      "\n",
      " EPOCH 2/100 \t train loss 40.10880661010742 \t \t val loss 34.30592727661133\n",
      "\n",
      " EPOCH 3/100 \t train loss 35.104549407958984 \t \t val loss 33.87270736694336\n",
      "\n",
      " EPOCH 4/100 \t train loss 33.375431060791016 \t \t val loss 32.57224655151367\n",
      "\n",
      " EPOCH 5/100 \t train loss 31.261940002441406 \t \t val loss 29.41970443725586\n",
      "\n",
      " EPOCH 6/100 \t train loss 29.91977882385254 \t \t val loss 28.000524520874023\n",
      "\n",
      " EPOCH 7/100 \t train loss 29.417606353759766 \t \t val loss 28.78483009338379\n",
      "\n",
      " EPOCH 8/100 \t train loss 29.22446632385254 \t \t val loss 30.296279907226562\n",
      "\n",
      " EPOCH 9/100 \t train loss 28.311710357666016 \t \t val loss 28.321138381958008\n",
      "\n",
      " EPOCH 10/100 \t train loss 27.47813606262207 \t \t val loss 25.83289909362793\n",
      "\n",
      " EPOCH 11/100 \t train loss 27.121959686279297 \t \t val loss 24.587621688842773\n",
      "\n",
      " EPOCH 12/100 \t train loss 25.971832275390625 \t \t val loss 25.952224731445312\n",
      "\n",
      " EPOCH 13/100 \t train loss 25.578125 \t \t val loss 24.854881286621094\n",
      "\n",
      " EPOCH 14/100 \t train loss 25.29523468017578 \t \t val loss 24.91914939880371\n",
      "\n",
      " EPOCH 15/100 \t train loss 24.91314125061035 \t \t val loss 27.350650787353516\n",
      "\n",
      " EPOCH 16/100 \t train loss 24.440439224243164 \t \t val loss 23.559036254882812\n",
      "\n",
      " EPOCH 17/100 \t train loss 24.32355499267578 \t \t val loss 24.01088523864746\n",
      "\n",
      " EPOCH 18/100 \t train loss 23.5207576751709 \t \t val loss 22.919374465942383\n",
      "\n",
      " EPOCH 19/100 \t train loss 23.429201126098633 \t \t val loss 22.18140983581543\n",
      "\n",
      " EPOCH 20/100 \t train loss 23.568206787109375 \t \t val loss 23.040882110595703\n",
      "\n",
      " EPOCH 21/100 \t train loss 22.979244232177734 \t \t val loss 21.996597290039062\n",
      "\n",
      " EPOCH 22/100 \t train loss 22.68025016784668 \t \t val loss 23.551542282104492\n",
      "\n",
      " EPOCH 23/100 \t train loss 22.241357803344727 \t \t val loss 21.306751251220703\n",
      "\n",
      " EPOCH 24/100 \t train loss 22.096742630004883 \t \t val loss 22.21418571472168\n",
      "\n",
      " EPOCH 25/100 \t train loss 21.906431198120117 \t \t val loss 21.28450584411621\n",
      "\n",
      " EPOCH 26/100 \t train loss 21.512985229492188 \t \t val loss 20.8778133392334\n",
      "\n",
      " EPOCH 27/100 \t train loss 21.663055419921875 \t \t val loss 20.78999900817871\n",
      "\n",
      " EPOCH 28/100 \t train loss 21.153879165649414 \t \t val loss 21.894100189208984\n",
      "\n",
      " EPOCH 29/100 \t train loss 21.37956428527832 \t \t val loss 21.06736946105957\n",
      "\n",
      " EPOCH 30/100 \t train loss 21.074661254882812 \t \t val loss 21.35870933532715\n",
      "\n",
      " EPOCH 31/100 \t train loss 21.0660343170166 \t \t val loss 20.50179100036621\n",
      "\n",
      " EPOCH 32/100 \t train loss 20.972698211669922 \t \t val loss 20.94233512878418\n",
      "\n",
      " EPOCH 33/100 \t train loss 20.538066864013672 \t \t val loss 22.019237518310547\n",
      "\n",
      " EPOCH 34/100 \t train loss 20.746511459350586 \t \t val loss 19.57876205444336\n",
      "\n",
      " EPOCH 35/100 \t train loss 20.636032104492188 \t \t val loss 19.548017501831055\n",
      "\n",
      " EPOCH 36/100 \t train loss 20.337263107299805 \t \t val loss 19.814651489257812\n",
      "\n",
      " EPOCH 37/100 \t train loss 20.32979393005371 \t \t val loss 20.93944549560547\n",
      "\n",
      " EPOCH 38/100 \t train loss 20.216657638549805 \t \t val loss 19.92302703857422\n",
      "\n",
      " EPOCH 39/100 \t train loss 20.12320327758789 \t \t val loss 19.91335105895996\n",
      "\n",
      " EPOCH 40/100 \t train loss 20.249954223632812 \t \t val loss 20.22479248046875\n",
      "\n",
      " EPOCH 41/100 \t train loss 20.000476837158203 \t \t val loss 19.833131790161133\n",
      "\n",
      " EPOCH 42/100 \t train loss 20.11844825744629 \t \t val loss 19.24037742614746\n",
      "\n",
      " EPOCH 43/100 \t train loss 19.81146240234375 \t \t val loss 19.71277618408203\n",
      "\n",
      " EPOCH 44/100 \t train loss 20.006921768188477 \t \t val loss 20.547290802001953\n",
      "\n",
      " EPOCH 45/100 \t train loss 19.875755310058594 \t \t val loss 20.801057815551758\n",
      "\n",
      " EPOCH 46/100 \t train loss 19.898954391479492 \t \t val loss 19.39947509765625\n",
      "\n",
      " EPOCH 47/100 \t train loss 19.849384307861328 \t \t val loss 19.43621826171875\n",
      "\n",
      " EPOCH 48/100 \t train loss 19.730571746826172 \t \t val loss 20.04530906677246\n",
      "\n",
      " EPOCH 49/100 \t train loss 19.734296798706055 \t \t val loss 18.934812545776367\n",
      "\n",
      " EPOCH 50/100 \t train loss 19.533092498779297 \t \t val loss 19.160602569580078\n",
      "\n",
      " EPOCH 51/100 \t train loss 19.551395416259766 \t \t val loss 19.65968894958496\n",
      "\n",
      " EPOCH 52/100 \t train loss 19.33845329284668 \t \t val loss 19.572519302368164\n",
      "\n",
      " EPOCH 53/100 \t train loss 19.48563003540039 \t \t val loss 19.046138763427734\n",
      "\n",
      " EPOCH 54/100 \t train loss 19.297990798950195 \t \t val loss 19.829641342163086\n",
      "\n",
      " EPOCH 55/100 \t train loss 19.233949661254883 \t \t val loss 20.516658782958984\n",
      "\n",
      " EPOCH 56/100 \t train loss 19.225000381469727 \t \t val loss 19.316036224365234\n",
      "\n",
      " EPOCH 57/100 \t train loss 18.99026107788086 \t \t val loss 18.419843673706055\n",
      "\n",
      " EPOCH 58/100 \t train loss 18.863309860229492 \t \t val loss 20.758262634277344\n",
      "\n",
      " EPOCH 59/100 \t train loss 18.872169494628906 \t \t val loss 19.057357788085938\n",
      "\n",
      " EPOCH 60/100 \t train loss 18.85643196105957 \t \t val loss 18.73960304260254\n",
      "\n",
      " EPOCH 61/100 \t train loss 18.909015655517578 \t \t val loss 18.662830352783203\n",
      "\n",
      " EPOCH 62/100 \t train loss 18.56673240661621 \t \t val loss 17.721797943115234\n",
      "\n",
      " EPOCH 63/100 \t train loss 18.56275177001953 \t \t val loss 18.570547103881836\n",
      "\n",
      " EPOCH 64/100 \t train loss 18.678298950195312 \t \t val loss 18.76418113708496\n",
      "\n",
      " EPOCH 65/100 \t train loss 18.466838836669922 \t \t val loss 18.696468353271484\n",
      "\n",
      " EPOCH 66/100 \t train loss 18.62189483642578 \t \t val loss 18.14105224609375\n",
      "\n",
      " EPOCH 67/100 \t train loss 18.31436538696289 \t \t val loss 19.06144905090332\n",
      "\n",
      " EPOCH 68/100 \t train loss 18.324859619140625 \t \t val loss 18.24460792541504\n",
      "\n",
      " EPOCH 69/100 \t train loss 18.206565856933594 \t \t val loss 18.44358253479004\n",
      "\n",
      " EPOCH 70/100 \t train loss 18.17494010925293 \t \t val loss 17.631933212280273\n",
      "\n",
      " EPOCH 71/100 \t train loss 18.100269317626953 \t \t val loss 17.542573928833008\n",
      "\n",
      " EPOCH 72/100 \t train loss 18.13180923461914 \t \t val loss 17.899097442626953\n",
      "\n",
      " EPOCH 73/100 \t train loss 18.046348571777344 \t \t val loss 17.91086769104004\n",
      "\n",
      " EPOCH 74/100 \t train loss 17.999710083007812 \t \t val loss 18.63046646118164\n",
      "\n",
      " EPOCH 75/100 \t train loss 18.04513168334961 \t \t val loss 17.72588539123535\n",
      "\n",
      " EPOCH 76/100 \t train loss 17.985647201538086 \t \t val loss 17.75813102722168\n",
      "\n",
      " EPOCH 77/100 \t train loss 17.834794998168945 \t \t val loss 17.679950714111328\n",
      "\n",
      " EPOCH 78/100 \t train loss 17.920392990112305 \t \t val loss 17.43556785583496\n",
      "\n",
      " EPOCH 79/100 \t train loss 17.96774673461914 \t \t val loss 19.35698699951172\n",
      "\n",
      " EPOCH 80/100 \t train loss 17.837148666381836 \t \t val loss 17.67340850830078\n",
      "\n",
      " EPOCH 81/100 \t train loss 17.922414779663086 \t \t val loss 17.571203231811523\n",
      "\n",
      " EPOCH 82/100 \t train loss 17.842634201049805 \t \t val loss 17.629316329956055\n",
      "\n",
      " EPOCH 83/100 \t train loss 17.886690139770508 \t \t val loss 17.974987030029297\n",
      "\n",
      " EPOCH 84/100 \t train loss 17.700231552124023 \t \t val loss 18.49613380432129\n",
      "\n",
      " EPOCH 85/100 \t train loss 17.774147033691406 \t \t val loss 17.940139770507812\n",
      "\n",
      " EPOCH 86/100 \t train loss 17.83566665649414 \t \t val loss 18.224050521850586\n",
      "\n",
      " EPOCH 87/100 \t train loss 17.80699920654297 \t \t val loss 17.35822296142578\n",
      "\n",
      " EPOCH 88/100 \t train loss 17.773839950561523 \t \t val loss 17.61142349243164\n",
      "\n",
      " EPOCH 89/100 \t train loss 17.54071044921875 \t \t val loss 17.026325225830078\n",
      "\n",
      " EPOCH 90/100 \t train loss 17.510906219482422 \t \t val loss 17.787315368652344\n",
      "\n",
      " EPOCH 91/100 \t train loss 17.68036460876465 \t \t val loss 17.180875778198242\n",
      "\n",
      " EPOCH 92/100 \t train loss 17.59282875061035 \t \t val loss 18.36480140686035\n",
      "\n",
      " EPOCH 93/100 \t train loss 17.594932556152344 \t \t val loss 17.529687881469727\n",
      "\n",
      " EPOCH 94/100 \t train loss 17.601158142089844 \t \t val loss 17.364042282104492\n",
      "\n",
      " EPOCH 95/100 \t train loss 17.64948844909668 \t \t val loss 18.11489486694336\n",
      "\n",
      " EPOCH 96/100 \t train loss 17.5512638092041 \t \t val loss 17.70476531982422\n",
      "\n",
      " EPOCH 97/100 \t train loss 17.601394653320312 \t \t val loss 17.69788360595703\n",
      "\n",
      " EPOCH 98/100 \t train loss 17.46657943725586 \t \t val loss 17.403705596923828\n",
      "\n",
      " EPOCH 99/100 \t train loss 17.443161010742188 \t \t val loss 16.83324432373047\n",
      "\n",
      " EPOCH 100/100 \t train loss 17.506078720092773 \t \t val loss 17.7973575592041\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1000\n",
    "num_epochs = 100\n",
    "diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(gcn, device, train_dataloader, loss_fn, optim)\n",
    "    test_loss = test(gcn, device, test_dataloader, loss_fn)\n",
    "    print('\\n EPOCH {}/{} \\t train loss {} \\t \\t val loss {}'.format(epoch + 1, num_epochs, train_loss, test_loss))\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(test_loss)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(gcn.state_dict(), \"./saved_models/with_resnet_{}.pth\".format(RUN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cdc9de2-1a6b-4df9-820e-edbee6326cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-03-08:49:14_lr=0.001_wd=1e-06_p=0.001_conv_features=128_n_layers=3_n_res=2\n"
     ]
    }
   ],
   "source": [
    "print(RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec127e87-5f02-42e4-a4bf-e3f3eb39f5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpi_python",
   "language": "python",
   "name": "mpi_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
